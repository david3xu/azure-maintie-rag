#!/usr/bin/env python3
"""
Data-driven knowledge quality validation and domain config generation

This script validates the quality of extracted knowledge from raw data
and generates domain configurations for the Azure Universal RAG system.

Purpose: Foundation for prompt flow and downstream tasks
"""

import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime

# Import Universal Agent for quality analysis
import sys
sys.path.append('.')
from agents.universal_agent import universal_agent

# Import unified data-driven configuration schema
from services.models.domain_models import (
    DataDrivenExtraction, DomainConfiguration, UnifiedDataDrivenConfig,
    DataDrivenConfigGenerator, DataExtractionQuality
)


class KnowledgeQualityValidator:
    """Validates and improves extracted knowledge quality for config generation"""

    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.processed_dir = self.data_dir / "processed"
        self.raw_dir = self.data_dir / "raw"
        self.config_dir = Path("config")
        self.config_generator = DataDrivenConfigGenerator(
            raw_data_path=str(self.raw_dir),
            config_output_path=str(self.config_dir)
        )

    def clean_processed_data(self):
        """Clean processed data to ensure reproducible pipeline"""
        print("ğŸ§¹ Cleaning processed data for reproducible pipeline...")

        if self.processed_dir.exists():
            import shutil
            shutil.rmtree(self.processed_dir)
            print("   âœ… Removed existing processed data")

        # Recreate processed directory
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        print("   âœ… Created fresh processed data directory")

    def validate_raw_data(self) -> bool:
        """Validate that raw data exists and is accessible"""
        print("ğŸ“‚ Validating raw data availability...")

        if not self.raw_dir.exists():
            print("   âŒ Raw data directory does not exist")
            return False

        # Check for files in root of raw data directory
        raw_files = list(self.raw_dir.glob("*.md")) + list(self.raw_dir.glob("*.txt"))

        # Also check subdirectories recursively
        raw_files.extend(list(self.raw_dir.rglob("*.md")))
        raw_files.extend(list(self.raw_dir.rglob("*.txt")))

        # Remove duplicates
        raw_files = list(set(raw_files))

        if not raw_files:
            print("   âŒ No raw data files found (.md or .txt)")
            return False

        print(f"   âœ… Found {len(raw_files)} raw data files")
        for file in raw_files[:3]:  # Show first 3 files
            print(f"      - {file.relative_to(self.raw_dir)}")

        return True

    async def analyze_knowledge_quality(self, knowledge_file: str) -> Dict[str, Any]:
        """Analyze the quality of extracted knowledge"""
        print(f"ğŸ” Analyzing knowledge quality from: {knowledge_file}")

        with open(knowledge_file, 'r') as f:
            knowledge_data = json.load(f)

        # Extract the knowledge for analysis
        extracted_knowledge = knowledge_data.get('extracted_knowledge', [])

        quality_analysis = {
            'total_files': knowledge_data.get('total_files', 0),
            'extraction_method': knowledge_data.get('extraction_method', 'unknown'),
            'quality_issues': [],
            'recommendations': [],
            'entity_quality': {},
            'domain_coverage': {},
            'readiness_for_config': False
        }

        for item in extracted_knowledge:
            if item.get('success'):
                entities = item.get('entities', [])
                domains = item.get('domains', [])
                relationships = item.get('relationships', [])

                # Analyze entity quality
                entity_issues = self._analyze_entity_quality(entities)
                quality_analysis['entity_quality'][item['filename']] = entity_issues

                # Analyze domain coverage
                domain_coverage = self._analyze_domain_coverage(domains, entities)
                quality_analysis['domain_coverage'][item['filename']] = domain_coverage

                # Collect quality issues
                if entity_issues['fragmented_entities'] > 0:
                    quality_analysis['quality_issues'].append(
                        f"Fragmented entities detected in {item['filename']}: {entity_issues['fragmented_entities']} items"
                    )

                if len(relationships) == 0:
                    quality_analysis['quality_issues'].append(
                        f"No relationships extracted from {item['filename']}"
                    )

                if len(domains) < 2:
                    quality_analysis['quality_issues'].append(
                        f"Insufficient domain coverage in {item['filename']}: only {len(domains)} domains"
                    )

        # Generate recommendations
        quality_analysis['recommendations'] = self._generate_quality_recommendations(quality_analysis)

        # Check readiness for config generation
        quality_analysis['readiness_for_config'] = len(quality_analysis['quality_issues']) == 0

        return quality_analysis

    def _analyze_entity_quality(self, entities: List[str]) -> Dict[str, Any]:
        """Analyze the quality of extracted entities"""
        fragmented_count = 0
        valid_entities = []

        for entity in entities:
            # Check for fragmented entities (newlines, short fragments)
            if '\n' in entity or len(entity.strip()) < 3:
                fragmented_count += 1
            else:
                valid_entities.append(entity)

        return {
            'total_entities': len(entities),
            'valid_entities': len(valid_entities),
            'fragmented_entities': fragmented_count,
            'quality_score': len(valid_entities) / len(entities) if entities else 0,
            'sample_valid': valid_entities[:5],
            'sample_fragmented': [e for e in entities if '\n' in e or len(e.strip()) < 3][:3]
        }

    def _analyze_domain_coverage(self, domains: List[str], entities: List[str]) -> Dict[str, Any]:
        """Analyze domain coverage and relevance"""
        return {
            'domains_detected': len(domains),
            'domains': domains,
            'entities_per_domain': len(entities) / len(domains) if domains else 0,
            'coverage_adequate': len(domains) >= 2 and len(entities) >= 10
        }

    def _generate_quality_recommendations(self, quality_analysis: Dict[str, Any]) -> List[str]:
        """Generate recommendations to improve knowledge quality"""
        recommendations = []

        if quality_analysis['quality_issues']:
            recommendations.append("âš ï¸ CRITICAL: Re-run knowledge extraction with improved prompts")
            recommendations.append("ğŸ”§ Use more specific entity extraction instructions")
            recommendations.append("ğŸ“Š Implement relationship extraction validation")

        if not quality_analysis['readiness_for_config']:
            recommendations.append("âŒ NOT READY for config generation - fix quality issues first")
        else:
            recommendations.append("âœ… READY for domain config generation")

        return recommendations

    async def improve_knowledge_extraction(self, raw_file: str) -> DataDrivenExtraction:
        """Re-extract knowledge with improved quality using unified schema"""
        print(f"ğŸ§  Improving knowledge extraction from: {raw_file}")

        raw_path = Path(raw_file)

        # Read raw content
        with open(raw_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # Improved extraction prompt
        extraction_prompt = f"""
        Analyze this Azure Machine Learning documentation and extract HIGH-QUALITY structured knowledge.

        REQUIREMENTS:
        1. Extract COMPLETE, MEANINGFUL entities (not fragments)
        2. Extract RELATIONSHIPS between entities
        3. Identify PRIMARY DOMAINS and SUBDOMAINS
        4. Extract KEY CONCEPTS and TECHNICAL TERMS
        5. Ensure entities are CLEAN (no newlines, proper names)

        Document content:
        {content[:10000]}...

        Return VALID JSON with this structure:
        {{
            "entities": ["Azure Machine Learning", "MLOps", "Data Science Workspace"],
            "relationships": [
                {{"source": "Azure Machine Learning", "relation": "provides", "target": "MLOps capabilities"}},
                {{"source": "Data Scientists", "relation": "use", "target": "Azure Machine Learning"}}
            ],
            "domains": ["cloud_computing", "machine_learning", "data_science", "developer_tools"],
            "key_concepts": ["AutoML", "Model Training", "Model Deployment", "Compute Instances"],
            "technical_terms": ["REST API", "Python SDK", "Azure CLI", "Jupyter Notebooks"]
        }}
        """

        # Try agent extraction first, with fallback to manual
        extracted_data = None
        extraction_method = "manual_fallback"

        try:
            # Use Universal Agent for improved extraction
            result = await universal_agent.run(extraction_prompt)
            response_text = result.output

            # Try to extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                extracted_data = json.loads(json_text)
                extraction_method = "universal_agent_improved"
                print(f"   âœ… Agent extraction successful")

        except Exception as e:
            print(f"   âš ï¸ Agent extraction failed: {e}")
            print(f"   ğŸ”„ Using manual extraction fallback")

        # Manual extraction fallback
        if extracted_data is None:
            extracted_data = self._manual_extraction_fallback(content)
            extraction_method = "manual_content_analysis"

        # Count fragmented entities
        fragmented_count = sum(
            1 for entity in extracted_data.get("entities", [])
            if '\n' in entity or len(entity.strip()) < 3
        )

        # Create DataDrivenExtraction object with automatic quality calculation
        extraction = DataDrivenExtraction(
            source_file=str(raw_path.relative_to(self.raw_dir)),
            content_size_bytes=len(content),
            extraction_method=extraction_method,
            entities=extracted_data.get("entities", []),
            relationships=extracted_data.get("relationships", []),
            domains=extracted_data.get("domains", []),
            key_concepts=extracted_data.get("key_concepts", []),
            technical_terms=extracted_data.get("technical_terms", []),
            fragmented_entities=fragmented_count
        )

        print(f"   âœ… Extraction quality: {extraction.extraction_quality.value} (score: {extraction.quality_score:.2f})")
        print(f"   ğŸ“Š Entities: {len(extraction.entities)}, Concepts: {len(extraction.key_concepts)}")
        print(f"   ğŸ”— Relationships: {len(extraction.relationships)}, Domains: {len(extraction.domains)}")

        return extraction

    def _manual_extraction_fallback(self, content: str) -> Dict[str, Any]:
        """Manual extraction fallback when agent is not available"""

        content_lower = content.lower()

        # Extract entities based on common patterns
        entities = []

        # Azure services and concepts
        azure_terms = [
            "Azure Machine Learning", "Azure ML", "Azure", "Microsoft Azure",
            "Azure OpenAI", "Azure Cognitive Search", "Azure Storage", "Azure Cosmos DB",
            "MLOps", "Machine Learning", "Data Science", "AutoML",
            "Python SDK", "REST API", "Azure CLI", "Jupyter Notebooks",
            "Compute Instances", "Model Training", "Model Deployment",
            "Datasets", "Experiments", "Pipelines", "Endpoints"
        ]

        for term in azure_terms:
            if term.lower() in content_lower:
                entities.append(term)

        # Extract key concepts
        key_concepts = [
            "Machine Learning", "Data Science", "MLOps", "AutoML",
            "Model Training", "Model Deployment", "Data Processing",
            "Cloud Computing", "AI/ML Pipeline", "Model Management"
        ]

        # Extract technical terms
        technical_terms = [
            "REST API", "Python SDK", "Azure CLI", "JSON", "YAML",
            "Docker", "Kubernetes", "Git", "CI/CD", "DevOps"
        ]

        # Extract domains dynamically from raw data structure
        from services.models.domain_models import discover_domains_from_directory
        discovered_domains = discover_domains_from_directory("data/raw")
        domains = discovered_domains + ["machine_learning", "data_science", "developer_tools"]

        # Create basic relationships
        relationships = [
            {"source": "Azure Machine Learning", "relation": "provides", "target": "MLOps capabilities"},
            {"source": "Data Scientists", "relation": "use", "target": "Azure Machine Learning"},
            {"source": "Azure ML", "relation": "supports", "target": "Python SDK"}
        ]

        return {
            "entities": entities[:15],  # Limit to reasonable number
            "relationships": relationships,
            "domains": domains,
            "key_concepts": key_concepts[:10],
            "technical_terms": technical_terms[:10]
        }

    async def generate_domain_config(self, improved_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Generate domain configuration from improved knowledge"""
        print("âš™ï¸ Generating domain configuration from knowledge...")

        # Extract domain information
        domains = improved_knowledge.get('domains', ['technology'])
        entities = improved_knowledge.get('entities', [])
        relationships = improved_knowledge.get('relationships', [])
        key_concepts = improved_knowledge.get('key_concepts', [])
        technical_terms = improved_knowledge.get('technical_terms', [])

        # Generate primary domain (most relevant)
        primary_domain = "azure_machine_learning" if any("azure" in d.lower() and "machine" in d.lower() for d in domains) else domains[0] if domains else "technology"

        # Create domain configuration
        domain_config = {
            "domain": {
                "name": primary_domain,
                "description": f"Domain configuration generated from Azure ML documentation",
                "version": "1.0.0",
                "generated_from_data": True,
                "generation_timestamp": datetime.now().isoformat(),
                "source_domains": domains
            },
            "entities": {
                "types": list(set([self._normalize_entity_type(e) for e in entities])),
                "primary_entities": entities[:10],  # Top entities
                "entity_count": len(entities),
                "extraction_confidence": 0.8 if len(entities) >= 5 else 0.6
            },
            "relationships": {
                "types": list(set([r.get("relation", "related_to") for r in relationships])),
                "relationship_patterns": [f"{r.get('source', '')} -> {r.get('relation', '')} -> {r.get('target', '')}" for r in relationships[:5]],
                "relationship_count": len(relationships)
            },
            "processing": {
                "chunk_size": 1000,
                "overlap": 200,
                "embedding_model": "text-embedding-ada-002",
                "extraction_model": "gpt-4.1",
                "quality_threshold": 0.7
            },
            "query": {
                "supported_query_types": ["factual", "procedural", "conceptual"],
                "key_concepts": key_concepts,
                "technical_vocabulary": technical_terms,
                "query_expansion_terms": entities[:20]
            },
            "performance": {
                "expected_response_time": 3.0,
                "cache_enabled": True,
                "cache_ttl": 3600,
                "batch_processing": True
            }
        }

        return domain_config

    def _normalize_entity_type(self, entity: str) -> str:
        """Normalize entity to a type classification"""
        entity_lower = entity.lower()

        if any(term in entity_lower for term in ["azure", "microsoft", "cloud"]):
            return "azure_service"
        elif any(term in entity_lower for term in ["ml", "machine learning", "model", "training"]):
            return "ml_concept"
        elif any(term in entity_lower for term in ["api", "sdk", "cli", "python"]):
            return "development_tool"
        elif any(term in entity_lower for term in ["data", "dataset", "scientist"]):
            return "data_concept"
        else:
            return "general_concept"

    async def save_domain_config(self, domain_config: Dict[str, Any]) -> str:
        """Save generated domain configuration"""
        domain_name = domain_config["domain"]["name"]
        config_file = self.config_dir / "domains" / f"{domain_name}.yaml"

        # Ensure directory exists
        config_file.parent.mkdir(parents=True, exist_ok=True)

        # Save as YAML
        import yaml
        with open(config_file, 'w') as f:
            yaml.safe_dump(domain_config, f, default_flow_style=False)

        print(f"ğŸ’¾ Domain config saved: {config_file}")
        return str(config_file)

    async def validate_for_prompt_flow(self, domain_config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate domain config readiness for prompt flow"""
        validation_results = {
            "prompt_flow_ready": False,
            "validation_checks": {},
            "recommendations": []
        }

        # Check required sections
        required_sections = ["domain", "entities", "relationships", "processing", "query", "performance"]
        for section in required_sections:
            validation_results["validation_checks"][f"{section}_present"] = section in domain_config

        # Check entity quality
        entities = domain_config.get("entities", {})
        validation_results["validation_checks"]["sufficient_entities"] = entities.get("entity_count", 0) >= 5
        validation_results["validation_checks"]["entity_confidence"] = entities.get("extraction_confidence", 0) >= 0.7

        # Check query capabilities
        query_config = domain_config.get("query", {})
        validation_results["validation_checks"]["query_concepts"] = len(query_config.get("key_concepts", [])) >= 3
        validation_results["validation_checks"]["query_vocabulary"] = len(query_config.get("technical_vocabulary", [])) >= 5

        # Overall readiness
        all_checks_passed = all(validation_results["validation_checks"].values())
        validation_results["prompt_flow_ready"] = all_checks_passed

        if all_checks_passed:
            validation_results["recommendations"].append("âœ… READY for prompt flow integration")
        else:
            failed_checks = [k for k, v in validation_results["validation_checks"].items() if not v]
            validation_results["recommendations"].append(f"âŒ Failed checks: {', '.join(failed_checks)}")

        return validation_results


async def main():
    """Main execution function"""
    print("ğŸš€ DATA-DRIVEN KNOWLEDGE QUALITY VALIDATION")
    print("=" * 60)
    print("Starting from raw data for complete reproducibility")
    print("=" * 60)

    # Ensure environment is loaded
    import os
    if not os.getenv('AZURE_OPENAI_ENDPOINT'):
        print("âš ï¸ Azure OpenAI not configured - using manual extraction")

    validator = KnowledgeQualityValidator()

    # Step 0: Ensure reproducible pipeline
    print("\nğŸ”„ STEP 0: Ensuring Reproducible Pipeline")

    # Validate raw data exists
    if not validator.validate_raw_data():
        print("âŒ Cannot proceed - raw data not available")
        return {"success": False, "error": "Raw data not found"}

    # Clean processed data for fresh start
    validator.clean_processed_data()

    # Step 1: Discover domains and process raw data from scratch
    print("\nğŸ§  STEP 1: Dynamic Domain Discovery and Knowledge Extraction")

    # Discover domains from directory structure (purely data-driven)
    from services.models.domain_models import discover_domains_from_directory, get_domain_files
    discovered_domains = discover_domains_from_directory("data/raw")
    print(f"\nğŸ¯ Processing {len(discovered_domains)} discovered domains: {discovered_domains}")
    print("ğŸ“‹ Note: Domain names used as-is from directory structure (no classification)")

    # Process each domain
    all_extractions = []
    for domain_name in discovered_domains:
        print(f"\nğŸ“ Processing domain: {domain_name}")
        domain_files = get_domain_files(domain_name, "data/raw")

        if not domain_files:
            print(f"   âš ï¸ No files found for domain: {domain_name}")
            continue

        print(f"   ğŸ“„ Found {len(domain_files)} files for {domain_name}")

        # Process each file in the domain
        for raw_file in domain_files[:1]:  # Start with first file per domain for validation
            print(f"      Processing: {raw_file.name}")
            extraction = await validator.improve_knowledge_extraction(str(raw_file))

            all_extractions.append(extraction)
            if extraction.is_suitable_for_config:
                print(f"      âœ… Suitable for config generation")
            else:
                print(f"      âš ï¸ Quality too low for config generation")

    if not all_extractions:
        print("âŒ No extractions completed")
        return {"success": False, "error": "No extractions completed"}

    # Step 2: Generate unified data-driven configuration
    print("\nâš™ï¸ STEP 2: Generating Unified Data-Driven Configuration")

    try:
        # Generate unified configuration using the schema
        unified_config = validator.config_generator.generate_unified_config(all_extractions)

        print(f"âœ… Unified config generated successfully!")
        print(f"ğŸ“Š Domain configurations: {len(unified_config.domain_configs)}")
        print(f"ğŸ” Data quality score: {unified_config.overall_readiness['data_quality_score']:.2f}")
        print(f"ğŸ¯ Competitive score: {unified_config.overall_readiness['competitive_score']:.2f}")

    except ValueError as e:
        print(f"âŒ Failed to generate unified config: {e}")
        return {"success": False, "error": str(e)}

    # Step 3: Save unified configuration
    print("\nğŸ’¾ STEP 3: Saving Unified Configuration")

    # Save to config directory
    config_file = validator.config_dir / "unified_data_driven_config.yaml"
    saved_config_path = unified_config.save_to_file(config_file)
    print(f"âœ… Unified config saved: {saved_config_path}")

    # Also save individual domain configs
    domains_dir = validator.config_dir / "domains"
    domains_dir.mkdir(exist_ok=True)

    for domain_name, domain_config in unified_config.domain_configs.items():
        domain_file = domains_dir / f"{domain_name}.yaml"
        import yaml
        with open(domain_file, 'w') as f:
            # Use mode='json' to serialize enums properly
            domain_dict = domain_config.model_dump(mode='json')
            yaml.safe_dump(domain_dict, f, default_flow_style=False)
        print(f"   ğŸ“ Domain config saved: {domain_file}")

    # Step 4: Validate prompt flow readiness
    print("\nğŸ” STEP 4: Validating Prompt Flow Readiness")

    overall_readiness = unified_config.overall_readiness
    print(f"âœ… Prompt Flow Ready: {overall_readiness['prompt_flow_ready']}")
    print(f"âœ… Production Ready: {overall_readiness['production_ready']}")
    print(f"ğŸ·ï¸ Domain Coverage: {overall_readiness['domain_coverage']} domains")
    print(f"ğŸ“Š Data Quality: {overall_readiness['data_quality_score']:.2f}")
    print(f"ğŸ¯ Competitive Score: {overall_readiness['competitive_score']:.2f}")

    # Print domain-specific readiness
    for domain_name, domain_config in unified_config.domain_configs.items():
        readiness = domain_config.prompt_flow_readiness
        status = "âœ…" if readiness['ready'] else "âš ï¸"
        print(f"   {status} {domain_name}: confidence {readiness['confidence_score']:.2f}")

    # Step 5: Reproducibility summary
    print("\nğŸ¯ STEP 5: Reproducibility Summary")
    print("=" * 60)

    print("ğŸ“‹ UNIFIED DATA-DRIVEN PIPELINE COMPLETED:")
    print(f"   ğŸ”„ Started from: data/raw (cleaned processed data)")
    print(f"   ğŸ“Š Schema-based extraction: {len(all_extractions)} files")
    print(f"   âš™ï¸ Unified config: {saved_config_path}")
    print(f"   ğŸ·ï¸ Domain configs: {len(unified_config.domain_configs)} generated")
    print(f"   âœ… Prompt flow ready: {overall_readiness['prompt_flow_ready']}")
    print(f"   ğŸš€ Production ready: {overall_readiness['production_ready']}")

    # Create comprehensive summary
    reproducibility_summary = {
        "pipeline_type": "unified_data_driven",
        "schema_version": "1.0.0",
        "pipeline_start": "data/raw",
        "processed_data_cleaned": True,
        "unified_config_file": str(saved_config_path),
        "domain_configs_generated": len(unified_config.domain_configs),
        "extractions_count": len(all_extractions),
        "suitable_extractions": len([e for e in all_extractions if e.is_suitable_for_config]),
        "overall_readiness": overall_readiness,
        "reproducible": True,
        "timestamp": datetime.now().isoformat()
    }

    # Save summary
    summary_file = "data/processed/unified_reproducibility_summary.json"
    with open(summary_file, 'w') as f:
        json.dump(reproducibility_summary, f, indent=2)

    print(f"ğŸ“Š Reproducibility summary: {summary_file}")

    if overall_readiness['production_ready']:
        print("\nğŸ‰ SUCCESS: Complete unified data-driven pipeline!")
        print("âœ… Production-ready configuration generated from raw data")
        print("âœ… Foundation prepared for prompt flow and downstream tasks")
        print("âœ… Centralized configuration schema implemented")
    else:
        print("\nâš ï¸ PARTIAL SUCCESS: Pipeline complete but quality improvements needed")

    return reproducibility_summary


if __name__ == "__main__":
    result = asyncio.run(main())
    print(f"\nğŸ Final Result: {result}")
