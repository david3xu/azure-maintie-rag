"""
Dual-Graph Workflow Orchestrator

This module implements the recommended dual-graph workflow architecture that cleanly
separates data processing (Config-Extraction) from query handling (Search) while
maintaining proper inter-graph communication and state management.

Architecture:
- Graph 1 (Config-Extraction): Corpus analysis and configuration generation
- Graph 2 (Search): Real-time query processing with learned configurations
- State Bridge: Manages data flow between graphs
- Configuration Manager: Centralized parameter loading without hardcoded values
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path

from agents.core.data_models import (
    WorkflowState,
    NodeState,
    WorkflowExecutionState,
    NodeExecutionResult,
)
from agents.workflows.config_extraction_graph import ConfigExtractionWorkflow
from agents.workflows.search_workflow_graph import SearchWorkflow
from agents.workflows.state_persistence import WorkflowStateManager
from agents.core.dynamic_config_manager import dynamic_config_manager

logger = logging.getLogger(__name__)


class DualGraphOrchestrator:
    """
    Orchestrates dual-graph workflow execution with clean separation of concerns.

    Key Features:
    - Clean boundaries between data processing and query handling
    - Zero-hardcoded-values through dynamic configuration loading
    - Proper inter-graph state management and communication
    - Adaptive workflow selection based on system state
    """

    def __init__(self):
        self.config_graph = ConfigExtractionWorkflow()
        self.search_graph = SearchWorkflow()
        self.state_manager = WorkflowStateManager()
        self.inter_graph_state = {}

    async def execute_config_extraction_pipeline(
        self, corpus_path: str, domain_name: str = None
    ) -> Dict[str, Any]:
        """
        Execute Graph 1: Data Processing Workflow

        This workflow should be run:
        1. When new documents are added to corpus
        2. When configuration needs updating
        3. For initial system setup

        Args:
            corpus_path: Path to document corpus
            domain_name: Target domain (auto-detected if None)

        Returns:
            Configuration generation results with learned parameters
        """

        workflow_id = f"config_extraction_{int(datetime.now().timestamp())}"

        logger.info(f"ðŸ”„ Starting Config-Extraction workflow for domain: {domain_name}")
        logger.info(f"ðŸ“‚ Analyzing corpus at: {corpus_path}")

        try:
            # Execute config extraction graph
            result = await self.config_graph.execute(
                {
                    "corpus_path": corpus_path,
                    "domain_name": domain_name,
                    "data_directory": corpus_path,
                    "workflow_type": "config_extraction",
                }
            )

            # Update inter-graph state for Search workflow
            if result["state"] == WorkflowState.COMPLETED.value:
                await self._update_inter_graph_state(
                    workflow_id, "config_extraction", result
                )

                logger.info(f"âœ… Config-Extraction workflow completed")
                logger.info(f"   ðŸ§  Domain configurations generated for: {domain_name}")
                logger.info(
                    f"   â±ï¸  Total execution time: {result['total_time_seconds']:.2f}s"
                )
            else:
                logger.error(
                    f"âŒ Config-Extraction workflow failed: {result.get('error', 'Unknown error')}"
                )

            return result

        except Exception as e:
            logger.error(f"âŒ Config-Extraction pipeline failed: {e}")
            await self.state_manager.save_workflow_state(
                workflow_id, WorkflowState.FAILED, {"error": str(e)}
            )
            raise

    async def execute_search_pipeline(
        self, query: str, domain: str = None, max_results: int = 10
    ) -> Dict[str, Any]:
        """
        Execute Graph 2: Query Handling Workflow

        This workflow handles real-time query processing using configurations
        generated by Graph 1 (Config-Extraction).

        Args:
            query: User query text
            domain: Target domain (auto-detected if None)
            max_results: Maximum results to return

        Returns:
            Search results with performance metrics
        """

        workflow_id = f"search_{int(datetime.now().timestamp())}"

        logger.info(f"ðŸ” Starting Search workflow for query: {query[:50]}...")
        if domain:
            logger.info(f"ðŸŽ¯ Target domain: {domain}")

        try:
            # Ensure domain configurations are available
            await self._ensure_domain_config_availability(domain or "general")

            # Execute search workflow
            result = await self.search_graph.execute(
                {
                    "query": query,
                    "domain": domain,
                    "max_results": max_results,
                    "workflow_type": "search",
                }
            )

            if result["state"] == WorkflowState.COMPLETED.value:
                logger.info(f"âœ… Search workflow completed")
                logger.info(
                    f"   ðŸ“Š Results found: {len(result.get('search_results', {}).get('results', []))}"
                )
                logger.info(
                    f"   â±ï¸  Total execution time: {result['total_time_seconds']:.2f}s"
                )
            else:
                logger.error(
                    f"âŒ Search workflow failed: {result.get('error', 'Unknown error')}"
                )

            return result

        except Exception as e:
            logger.error(f"âŒ Search pipeline failed: {e}")
            await self.state_manager.save_workflow_state(
                workflow_id, WorkflowState.FAILED, {"error": str(e)}
            )
            raise

    async def execute_full_pipeline(
        self, corpus_path: str, query: str = None, domain_name: str = None
    ) -> Dict[str, Any]:
        """
        Execute both graphs in sequence: Config-Extraction â†’ Search

        Use this for:
        1. Initial system setup
        2. Complete corpus processing with immediate search
        3. Integration testing

        Args:
            corpus_path: Path to document corpus
            query: Optional query to execute after config generation
            domain_name: Target domain

        Returns:
            Combined results from both workflows
        """

        logger.info(f"ðŸš€ Starting full dual-graph pipeline")

        results = {
            "config_extraction_result": None,
            "search_result": None,
            "pipeline_start_time": datetime.now(),
            "pipeline_end_time": None,
            "total_pipeline_time": WorkflowConstants.ZERO_PIPELINE_TIME,
        }

        try:
            # Step 1: Execute Config-Extraction workflow
            config_result = await self.execute_config_extraction_pipeline(
                corpus_path, domain_name
            )
            results["config_extraction_result"] = config_result

            if config_result["state"] != WorkflowState.COMPLETED.value:
                raise Exception(
                    f"Config-Extraction workflow failed: {config_result.get('error')}"
                )

            # Step 2: Execute Search workflow if query provided
            if query:
                search_result = await self.execute_search_pipeline(query, domain_name)
                results["search_result"] = search_result

            results["pipeline_end_time"] = datetime.now()
            results["total_pipeline_time"] = (
                results["pipeline_end_time"] - results["pipeline_start_time"]
            ).total_seconds()

            logger.info(f"âœ… Full dual-graph pipeline completed")
            logger.info(
                f"   â±ï¸  Total pipeline time: {results['total_pipeline_time']:.2f}s"
            )

            return results

        except Exception as e:
            results["pipeline_end_time"] = datetime.now()
            results["total_pipeline_time"] = (
                results["pipeline_end_time"] - results["pipeline_start_time"]
            ).total_seconds()
            results["error"] = str(e)

            logger.error(f"âŒ Full pipeline failed: {e}")
            return results

    async def _ensure_domain_config_availability(self, domain: str):
        """
        Ensure domain-specific configuration is available for Search workflow.

        If no configuration exists, trigger Config-Extraction workflow.
        """

        try:
            # Try to load existing configuration
            config = await dynamic_config_manager.get_extraction_config(domain)
            logger.info(f"ðŸ“‹ Loaded existing configuration for domain: {domain}")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  No configuration found for domain '{domain}': {e}")
            logger.info(f"ðŸ”„ Auto-triggering Config-Extraction workflow")

            # Auto-trigger Config-Extraction workflow
            corpus_path = f"/workspace/azure-maintie-rag/data/raw/{domain.replace('_', '-').title()}"
            if Path(corpus_path).exists():
                await self.execute_config_extraction_pipeline(corpus_path, domain)
            else:
                logger.error(f"âŒ Corpus path not found: {corpus_path}")
                raise Exception(
                    f"Cannot generate config for domain '{domain}' - corpus not found"
                )

    async def _update_inter_graph_state(
        self, workflow_id: str, workflow_type: str, result: Dict[str, Any]
    ):
        """Update inter-graph state for communication between workflows"""

        self.inter_graph_state[workflow_type] = {
            "workflow_id": workflow_id,
            "timestamp": datetime.now(),
            "result": result,
            "status": "completed",
        }

        # Save to persistent storage for cross-session availability
        await self.state_manager.save_workflow_state(
            f"inter_graph_{workflow_type}",
            WorkflowState.COMPLETED,
            self.inter_graph_state[workflow_type],
        )

    async def get_pipeline_status(self) -> Dict[str, Any]:
        """Get status of both workflow graphs"""

        return {
            "config_extraction_graph": {
                "status": "operational",
                "last_execution": self.inter_graph_state.get(
                    "config_extraction", {}
                ).get("timestamp"),
            },
            "search_graph": {
                "status": "operational",
                "last_execution": self.inter_graph_state.get("search", {}).get(
                    "timestamp"
                ),
            },
            "inter_graph_communication": "active",
            "dynamic_config_manager": "operational",
        }


# Global orchestrator instance
dual_graph_orchestrator = DualGraphOrchestrator()

# Export main components
__all__ = ["DualGraphOrchestrator", "dual_graph_orchestrator"]
