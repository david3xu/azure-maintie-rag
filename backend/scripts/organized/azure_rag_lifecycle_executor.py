#!/usr/bin/env python3
"""
Azure Universal RAG Lifecycle Executor
Complete end-to-end pipeline execution following AZURE_RAG_EXECUTION_PLAN.md

Steps:
0. Azure Data Cleanup
1. Data Upload & Chunking  
2. Knowledge Extraction
3. Knowledge Graph Loading
4. GNN Training Preparation
5. GNN Training Execution
6. Multi-hop Reasoning
7. End-to-End Query Processing
8. Real Knowledge Graph Operations

Usage:
    python azure_rag_lifecycle_executor.py --steps all
    python azure_rag_lifecycle_executor.py --steps 0,1,2
    python azure_rag_lifecycle_executor.py --step 3
"""

import asyncio
import json
import os
import sys
import time
import argparse
from datetime import datetime
from pathlib import Path

# Add backend to path
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

class AzureRAGLifecycleExecutor:
    """Execute complete Azure RAG lifecycle with data state tracking"""
    
    def __init__(self, session_id=None):
        self.session_id = session_id or f"lifecycle_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.execution_log = {
            "session_id": self.session_id,
            "start_time": datetime.now().isoformat(),
            "steps": {},
            "data_states": {},
            "execution_plan_version": "2025-07-27_production"
        }
        print(f"üöÄ Azure RAG Lifecycle Executor - Session: {self.session_id}")
        
    def log_step_start(self, step_num, step_name, script_path):
        """Log the start of a lifecycle step"""
        print(f"\n{'='*60}")
        print(f"üîÑ STEP {step_num}: {step_name}")
        print(f"üìú Script: {script_path}")
        print(f"‚è∞ Started: {datetime.now().strftime('%H:%M:%S')}")
        print('='*60)
        
        self.execution_log["steps"][f"step_{step_num}"] = {
            "name": step_name,
            "script": script_path,
            "start_time": datetime.now().isoformat(),
            "status": "running"
        }
        
    def log_step_complete(self, step_num, duration, success=True, data_state=None, notes=None):
        """Log the completion of a lifecycle step"""
        status = "‚úÖ SUCCESS" if success else "‚ùå FAILED"
        print(f"\n{status} - Step {step_num} completed in {duration:.1f}s")
        if notes:
            print(f"üìù Notes: {notes}")
        if data_state:
            print(f"üìä Data State: {data_state}")
            
        self.execution_log["steps"][f"step_{step_num}"].update({
            "end_time": datetime.now().isoformat(),
            "duration_seconds": duration,
            "status": "success" if success else "failed",
            "data_state": data_state,
            "notes": notes
        })
        
    def check_azure_data_state(self):
        """Check current Azure data state"""
        try:
            from integrations.azure_services import AzureServicesManager
            services = AzureServicesManager()
            # This would be implemented to check actual Azure state
            return {
                "blob_storage": "checked",
                "cognitive_search": "checked", 
                "cosmos_db": "checked",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
    
    async def step_0_azure_cleanup(self):
        """Step 0: Clean Azure data state"""
        start_time = time.time()
        self.log_step_start(0, "Azure Data Cleanup", "workflows/azure_data_cleanup_workflow.py")
        
        try:
            # Execute data cleanup
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/workflows/azure_data_cleanup_workflow.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            data_state = self.check_azure_data_state()
            duration = time.time() - start_time
            
            notes = "All Azure services cleaned and ready" if success else f"Cleanup failed: {result.stderr[:100]}"
            self.log_step_complete(0, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(0, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_1_data_upload(self):
        """Step 1: Data Upload & Chunking"""
        start_time = time.time()
        self.log_step_start(1, "Data Upload & Chunking", "data_processing/data_upload_workflow.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/data_processing/data_upload_workflow.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            data_state = {"documents_uploaded": "checked", "chunks_created": "checked"}
            duration = time.time() - start_time
            
            notes = "Documents uploaded and chunked successfully" if success else "Upload failed"
            self.log_step_complete(1, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(1, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_2_knowledge_extraction(self):
        """Step 2: Knowledge Extraction"""
        start_time = time.time()
        self.log_step_start(2, "Knowledge Extraction", "data_processing/full_dataset_extraction.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/data_processing/full_dataset_extraction.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            
            # Check extraction output
            extraction_files = list(Path(backend_path / "data/extraction_outputs").glob("full_dataset_extraction_*.json"))
            latest_extraction = max(extraction_files, key=os.path.getctime) if extraction_files else None
            
            if latest_extraction:
                with open(latest_extraction) as f:
                    extraction_data = json.load(f)
                data_state = {
                    "entities_extracted": len(extraction_data.get("entities", [])),
                    "relationships_extracted": len(extraction_data.get("relationships", [])),
                    "extraction_file": str(latest_extraction)
                }
            else:
                data_state = {"entities_extracted": 0, "relationships_extracted": 0}
            
            duration = time.time() - start_time
            notes = f"Extracted {data_state.get('entities_extracted', 0)} entities, {data_state.get('relationships_extracted', 0)} relationships"
            self.log_step_complete(2, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(2, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_3_knowledge_graph_loading(self):
        """Step 3: Knowledge Graph Loading to Azure Cosmos DB"""
        start_time = time.time()
        self.log_step_start(3, "Knowledge Graph Loading", "data_processing/azure_kg_bulk_loader.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/data_processing/azure_kg_bulk_loader.py", 
                "--max-entities", "1000"  # Load subset for demo
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            
            # Check loading results
            loading_files = list(Path(backend_path / "data/loading_results").glob("azure_kg_load_*.json"))
            latest_loading = max(loading_files, key=os.path.getctime) if loading_files else None
            
            if latest_loading:
                with open(latest_loading) as f:
                    loading_data = json.load(f)
                data_state = {
                    "entities_loaded": loading_data.get("entities_loaded", 0),
                    "relationships_loaded": loading_data.get("relationships_loaded", 0),
                    "loading_file": str(latest_loading)
                }
            else:
                data_state = {"entities_loaded": 0, "relationships_loaded": 0}
            
            duration = time.time() - start_time
            notes = f"Loaded {data_state.get('entities_loaded', 0)} entities, {data_state.get('relationships_loaded', 0)} relationships to Azure Cosmos DB"
            self.log_step_complete(3, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(3, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_4_gnn_training_prep(self):
        """Step 4: GNN Training Preparation"""
        start_time = time.time()
        self.log_step_start(4, "GNN Training Preparation", "gnn_training/prepare_gnn_training_features.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/gnn_training/prepare_gnn_training_features.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            
            # Check training data
            training_files = list(Path(backend_path / "data/gnn_training").glob("gnn_training_data_*.npz"))
            latest_training = max(training_files, key=os.path.getctime) if training_files else None
            
            if latest_training:
                import numpy as np
                training_data = np.load(latest_training)
                data_state = {
                    "node_features_shape": list(training_data['node_features'].shape),
                    "edge_index_shape": list(training_data['edge_index'].shape),
                    "training_file": str(latest_training)
                }
            else:
                data_state = {"training_data_prepared": False}
            
            duration = time.time() - start_time
            notes = f"GNN training features prepared: {data_state.get('node_features_shape', 'N/A')} node features"
            self.log_step_complete(4, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(4, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_5_gnn_training(self):
        """Step 5: GNN Training Execution"""
        start_time = time.time()
        self.log_step_start(5, "GNN Training Execution", "gnn_training/real_gnn_training_azure.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/gnn_training/real_gnn_training_azure.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            
            # Check model outputs
            model_files = list(Path(backend_path / "data/gnn_models").glob("real_gnn_model_*.json"))
            latest_model = max(model_files, key=os.path.getctime) if model_files else None
            
            if latest_model:
                with open(latest_model) as f:
                    model_data = json.load(f)
                data_state = {
                    "model_accuracy": model_data.get("test_accuracy", 0),
                    "model_parameters": model_data.get("total_parameters", 0),
                    "model_file": str(latest_model)
                }
            else:
                data_state = {"model_trained": False}
            
            duration = time.time() - start_time
            notes = f"GNN model trained with {data_state.get('model_accuracy', 0):.3f} accuracy, {data_state.get('model_parameters', 0)} parameters"
            self.log_step_complete(5, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(5, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_6_multi_hop_reasoning(self):
        """Step 6: Multi-hop Reasoning"""
        start_time = time.time()
        self.log_step_start(6, "Multi-hop Reasoning", "workflows/multi_hop_reasoning.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/workflows/multi_hop_reasoning.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            duration = time.time() - start_time
            
            data_state = {"multi_hop_reasoning": "tested", "graph_traversal": "working"}
            notes = "Multi-hop reasoning capabilities demonstrated"
            self.log_step_complete(6, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(6, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_7_query_processing(self):
        """Step 7: End-to-End Query Processing"""
        start_time = time.time()
        self.log_step_start(7, "End-to-End Query Processing", "workflows/query_processing_workflow.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/workflows/query_processing_workflow.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            duration = time.time() - start_time
            
            data_state = {"api_functional": success, "azure_services_integrated": success}
            notes = "End-to-end query processing pipeline tested"
            self.log_step_complete(7, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(7, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def step_8_kg_operations(self):
        """Step 8: Real Knowledge Graph Operations"""
        start_time = time.time()
        self.log_step_start(8, "Real Knowledge Graph Operations", "data_processing/azure_real_kg_operations.py")
        
        try:
            import subprocess
            result = subprocess.run([
                sys.executable, "scripts/organized/data_processing/azure_real_kg_operations.py"
            ], capture_output=True, text=True, cwd=backend_path)
            
            success = result.returncode == 0
            
            # Check KG operations results
            kg_files = list(Path(backend_path / "data/kg_operations").glob("azure_real_kg_demo.json"))
            if kg_files:
                with open(kg_files[0]) as f:
                    kg_data = json.load(f)
                data_state = {
                    "graph_traversal_working": kg_data.get("graph_traversal", {}).get("status") == "success",
                    "semantic_search_working": kg_data.get("semantic_search", {}).get("status") == "success",
                    "operations_file": str(kg_files[0])
                }
            else:
                data_state = {"kg_operations": "checked"}
            
            duration = time.time() - start_time
            notes = "Real knowledge graph operations demonstrated successfully"
            self.log_step_complete(8, duration, success, data_state, notes)
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.log_step_complete(8, duration, False, None, f"Exception: {str(e)}")
            return False
    
    async def execute_lifecycle(self, steps_to_run="all"):
        """Execute the complete or partial Azure RAG lifecycle"""
        print(f"üöÄ Starting Azure RAG Lifecycle Execution")
        print(f"üìã Session: {self.session_id}")
        print(f"üéØ Steps: {steps_to_run}")
        
        if steps_to_run == "all":
            steps = list(range(9))  # 0-8
        elif isinstance(steps_to_run, str):
            steps = [int(s.strip()) for s in steps_to_run.split(",")]
        else:
            steps = [steps_to_run]
        
        step_functions = {
            0: self.step_0_azure_cleanup,
            1: self.step_1_data_upload,
            2: self.step_2_knowledge_extraction,
            3: self.step_3_knowledge_graph_loading,
            4: self.step_4_gnn_training_prep,
            5: self.step_5_gnn_training,
            6: self.step_6_multi_hop_reasoning,
            7: self.step_7_query_processing,
            8: self.step_8_kg_operations
        }
        
        overall_start = time.time()
        successful_steps = 0
        
        for step_num in steps:
            if step_num in step_functions:
                success = await step_functions[step_num]()
                if success:
                    successful_steps += 1
                else:
                    print(f"‚ö†Ô∏è  Step {step_num} failed - continuing to next step")
            else:
                print(f"‚ùå Invalid step number: {step_num}")
        
        # Final summary
        total_duration = time.time() - overall_start
        self.execution_log["end_time"] = datetime.now().isoformat()
        self.execution_log["total_duration_seconds"] = total_duration
        self.execution_log["success_rate"] = successful_steps / len(steps)
        self.execution_log["successful_steps"] = successful_steps
        self.execution_log["total_steps"] = len(steps)
        
        # Save execution log
        log_file = f"/workspace/azure-maintie-rag/backend/data/demo_outputs/azure_rag_lifecycle_{self.session_id}.json"
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        with open(log_file, 'w') as f:
            json.dump(self.execution_log, f, indent=2)
        
        print(f"\n{'='*60}")
        print(f"üèÅ Azure RAG Lifecycle Execution Complete")
        print(f"‚è±Ô∏è  Total Duration: {total_duration:.1f} seconds")
        print(f"‚úÖ Success Rate: {self.execution_log['success_rate']:.1%}")
        print(f"üìä Steps Completed: {successful_steps}/{len(steps)}")
        print(f"üíæ Execution Log: {log_file}")
        print('='*60)
        
        return self.execution_log

async def main():
    parser = argparse.ArgumentParser(description="Azure RAG Lifecycle Executor")
    parser.add_argument("--steps", default="all", help="Steps to run: 'all', '0,1,2', or single step number")
    parser.add_argument("--session-id", help="Custom session ID")
    
    args = parser.parse_args()
    
    executor = AzureRAGLifecycleExecutor(session_id=args.session_id)
    await executor.execute_lifecycle(args.steps)

if __name__ == "__main__":
    asyncio.run(main())