#!/usr/bin/env python3
"""
Data Upload and Chunking Workflow Script
========================================

STEP 1 of Azure Universal RAG Pipeline
Handles document upload and intelligent chunking using Azure services.

Azure Services Used:
- Azure Blob Storage (store documents and chunks)
- Azure OpenAI (intelligent document analysis and chunking)
- Azure Cognitive Search (create search indices for chunks)
"""

import sys
import asyncio
import time
from pathlib import Path
from datetime import datetime
import json
import logging

# Configure logging for better error visibility
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add backend to path
backend_path = Path(__file__).parent.parent
sys.path.insert(0, str(backend_path))

# Import Azure services architecture components
from integrations.azure_services import AzureServicesManager
from integrations.azure_openai import AzureOpenAIClient
from config.settings import AzureSettings
from core.workflow.data_workflow_evidence import AzureDataWorkflowEvidenceCollector
from core.workflow.cost_tracker import AzureServiceCostTracker
from core.workflow.progress_tracker import create_progress_tracker, track_async_operation, track_sync_operation
from core.utilities.intelligent_document_processor import UniversalDocumentProcessor

azure_settings = AzureSettings()

def _detect_content_type(filename: str) -> str:
    """Detect content type from filename extension"""
    file_ext = Path(filename).suffix.lower()
    content_type_mapping = {
        '.md': 'markdown',
        '.txt': 'text'
    }
    return content_type_mapping.get(file_ext, 'text')

def load_raw_data_from_directory(data_dir: str = "data/raw") -> list:
    """Load all supported text files from the raw data directory"""
    from config.settings import azure_settings

    raw_data_path = Path(data_dir)
    if not raw_data_path.exists():
        print(f"‚ùå Raw data directory not found: {raw_data_path}")
        return []

    # Get supported file patterns from configuration
    supported_patterns = getattr(azure_settings, 'raw_data_include_patterns', ["*.md", "*.txt"])

    # Find all supported text files
    all_text_files = []
    for pattern in supported_patterns:
        files = list(raw_data_path.glob(pattern))
        all_text_files.extend(files)

    if not all_text_files:
        supported_formats = ", ".join(supported_patterns)
        print(f"‚ùå No supported text files found in {raw_data_path}")
        print(f"üìã Supported formats: {supported_formats}")
        return []

    # Sort by filename for consistent processing order
    all_text_files.sort(key=lambda x: x.name)

    documents = []
    for file_path in all_text_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            documents.append({
                'filename': file_path.name,
                'content': content,
                'size': len(content),
                'path': str(file_path),
                'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            })
            file_type = _detect_content_type(file_path.name)
            print(f"üìÑ Loaded: {file_path.name} ({len(content)} characters, {file_type})")

        except Exception as e:
            print(f"‚ùå Error loading {file_path.name}: {e}")

    return documents

async def main():
    """Execute data upload and chunking workflow with Azure services"""

    # Initialize real-time progress tracker
    progress_tracker = create_progress_tracker("Azure Data Upload & Chunking")
    progress_tracker.start_workflow()

    # Initialize workflow tracking
    start_time = time.time()
    domain = sys.argv[1] if len(sys.argv) > 1 else "general"

    try:
        # Load raw data from data/raw directory
        progress_tracker.start_step("Data Loading", {"directory": "data/raw"})
        raw_documents = load_raw_data_from_directory()

        if not raw_documents:
            progress_tracker.complete_step("Data Loading", success=False, error_message="No raw documents found")
            print("‚ùå No raw documents found. Please add markdown files to data/raw/")
            return 1

        progress_tracker.update_step_progress("Data Loading", {"documents_loaded": len(raw_documents)})
        progress_tracker.complete_step("Data Loading", success=True)
        print(f"‚úÖ Loaded {len(raw_documents)} documents from data/raw/")

        # Initialize Azure services
        print(f"\nüìù Initializing Azure services...")
        azure_services = AzureServicesManager()

        # Validate services
        print(f"\nüìù Validating Azure services configuration...")
        validation = azure_services.validate_configuration()
        if not validation['all_configured']:
            raise RuntimeError(f"Azure services not properly configured: {validation}")

        openai_integration = AzureOpenAIClient()

        # Use get_rag_storage_client for storage
        rag_storage = azure_services.get_rag_storage_client()
        search_client = azure_services.get_service('search')

        # Step 1: Store original documents in Azure Blob Storage
        progress_tracker.start_step("Blob Storage", {"container": f"raw-documents-{domain}", "documents": len(raw_documents)})
        original_container = f"raw-documents-{domain}"
        await rag_storage.create_container(original_container)

        uploaded_count = 0
        for doc in raw_documents:
            blob_name = f"original/{doc['filename']}"
            await rag_storage.upload_text(original_container, blob_name, doc['content'])
            uploaded_count += 1
            progress_tracker.update_step_progress("Blob Storage", {"uploaded": uploaded_count, "total": len(raw_documents)})
            print(f"   üìÑ Uploaded: {doc['filename']}")

        progress_tracker.complete_step("Blob Storage", success=True)

        # Step 2: Intelligent document chunking
        progress_tracker.start_step("Knowledge Extraction", {"documents": len(raw_documents)})

        doc_processor = UniversalDocumentProcessor(
            azure_openai_client=openai_integration,
            max_chunk_size=2000,  # Optimal size for knowledge extraction
            overlap_size=200
        )

        # Process all documents into intelligent chunks
        progress_tracker.update_step_progress("Knowledge Extraction", {"status": "Processing documents with intelligent chunking"})
        all_chunks = []
        total_chunks_created = 0

        for doc in raw_documents:
            print(f"   üîç Processing: {doc['filename']}")
            chunks = await doc_processor.process_document(doc)
            all_chunks.extend(chunks)
            total_chunks_created += len(chunks)
            progress_tracker.update_step_progress("Knowledge Extraction", {
                "documents_processed": len(all_chunks),
                "total_documents": len(raw_documents),
                "chunks_created": total_chunks_created
            })
            print(f"   ‚úÖ Created {len(chunks)} intelligent chunks from {doc['filename']}")

        print(f"\nüìä Chunking Summary:")
        print(f"   üìÑ Original documents: {len(raw_documents)}")
        print(f"   üß© Total chunks created: {total_chunks_created}")
        print(f"   üìà Average chunks per document: {total_chunks_created/len(raw_documents):.1f}")

        # Step 3: Store chunks in Azure Blob Storage
        progress_tracker.start_step("Cosmos Storage", {"chunks": len(all_chunks), "container": f"processed-chunks-{domain}"})
        chunks_container = f"processed-chunks-{domain}"
        await rag_storage.create_container(chunks_container)

        chunks_metadata = []
        stored_count = 0
        for i, chunk in enumerate(all_chunks):
            # Store chunk content
            chunk_blob_name = f"chunk_{i:04d}_{chunk.source_info['filename']}_{chunk.chunk_index}.txt"
            await rag_storage.upload_text(chunks_container, chunk_blob_name, chunk.content)

            # Store chunk metadata
            chunk_metadata = {
                "chunk_id": f"chunk_{i:04d}",
                "blob_name": chunk_blob_name,
                "source_file": chunk.source_info['filename'],
                "chunk_index": chunk.chunk_index,
                "chunk_type": chunk.chunk_type,
                "content_length": len(chunk.content),
                "metadata": chunk.metadata,
                "processing_timestamp": datetime.now().isoformat()
            }
            chunks_metadata.append(chunk_metadata)
            stored_count += 1

            progress_tracker.update_step_progress("Cosmos Storage", {
                "stored": stored_count,
                "total": len(all_chunks),
                "progress": f"{stored_count}/{len(all_chunks)}"
            })

            if i % 20 == 0:
                print(f"   üì¶ Stored chunk {i+1}/{len(all_chunks)}")

        # Store chunks index file
        chunks_index = {
            "domain": domain,
            "total_chunks": len(all_chunks),
            "original_documents": len(raw_documents),
            "created_at": datetime.now().isoformat(),
            "chunks": chunks_metadata
        }

        index_blob_name = f"chunks_index_{domain}.json"
        await rag_storage.upload_text(chunks_container, index_blob_name, json.dumps(chunks_index, indent=2))
        print(f"   üìã Stored chunks index: {index_blob_name}")

        progress_tracker.complete_step("Cosmos Storage", success=True)

        # Step 4: Create search index for chunks (without chunk_type field to avoid schema issues)
        progress_tracker.start_step("Search Indexing", {"chunks": len(all_chunks), "index": f"chunks-index-{domain}"})
        index_name = f"chunks-index-{domain}"
        await search_client.create_index(index_name)

        success_count = 0
        for i, chunk in enumerate(all_chunks):
            # Create search document from chunk (excluding problematic fields)
            document = {
                "id": f"chunk_{i:04d}_{chunk.source_info['filename'].replace('.md', '').replace('.txt', '')}_{chunk.chunk_index}",
                "content": chunk.content,
                "title": f"{chunk.source_info['filename']} - Part {chunk.chunk_index + 1}",
                "domain": domain,
                "source": f"data/raw/{chunk.source_info['filename']}",
                "metadata": json.dumps({
                    "original_filename": chunk.source_info['filename'],
                    "chunk_index": chunk.chunk_index,
                    "chunk_type": chunk.chunk_type,
                    "total_chunks": chunk.metadata['total_chunks'],
                    "processing_method": chunk.metadata['processing_method'],
                    "file_size": chunk.source_info['size'],
                    "processing_timestamp": datetime.now().isoformat(),
                    "content_type": _detect_content_type(chunk.source_info['filename']),
                    "intelligent_chunking": True,
                    "workflow_step": "upload_and_chunking"
                })
            }

            try:
                index_result = await search_client.index_document(index_name, document)
                if index_result['success']:
                    success_count += 1
                    progress_tracker.update_step_progress("Search Indexing", {
                        "indexed": success_count,
                        "total": len(all_chunks),
                        "progress": f"{success_count}/{len(all_chunks)}"
                    })
                else:
                    logger.error(f"Failed to index chunk {document['id']}: {index_result.get('error', 'Unknown error')}")
            except Exception as e:
                logger.error(f"Error indexing chunk {document['id']}: {str(e)}")

        progress_tracker.complete_step("Search Indexing", success=True)

        # Final summary
        progress_tracker.finish_workflow(success=True)
        duration = time.time() - start_time
        print(f"\nüìä STEP 1 Completion Summary:")
        print(f"   üìÑ Documents processed: {len(raw_documents)}")
        print(f"   üß© Chunks created: {len(all_chunks)}")
        print(f"   ‚òÅÔ∏è  Chunks stored in Azure: {len(all_chunks)}")
        print(f"   üîç Chunks indexed: {success_count}/{len(all_chunks)}")
        print(f"   ‚ö° Success rate: {(success_count/len(all_chunks)*100):.1f}%")
        print(f"   ‚è±Ô∏è  Total duration: {duration:.1f} seconds")
        print()
        print(f"‚úÖ STEP 1 COMPLETED: Data upload and chunking ready for knowledge extraction")
        print(f"üîÑ Next step: Run 'make knowledge-extract' to extract entities and relations")

        if success_count == 0:
            print(f"\n‚ùå Critical: Zero chunks indexed in Azure Search")
            return 1

        return 0

    except Exception as e:
        if 'progress_tracker' in locals():
            progress_tracker.finish_workflow(success=False)
        print(f"\n‚ùå Critical error in data upload workflow: {str(e)}")
        logger.error(f"Data upload workflow failed: {str(e)}", exc_info=True)
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)