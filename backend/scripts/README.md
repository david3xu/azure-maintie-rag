# Backend Scripts

**Architecture**: Organized by data flow stages that directly reflect README pipeline

**Current Status**: âœ… Data flow-aligned architecture implemented

---

## ğŸ“ **Script Organization**

```
backend/scripts/
â”œâ”€â”€ dataflow/                    # ğŸ”„ CURRENT - Data flow stage scripts
â”‚   â”œâ”€â”€ 01_data_ingestion.py     # Raw text â†’ Blob Storage
â”‚   â”œâ”€â”€ 02_knowledge_extraction.py # Blob â†’ Knowledge extraction  
â”‚   â”œâ”€â”€ 03_vector_indexing.py    # Text â†’ Vector embeddings
â”‚   â”œâ”€â”€ 04_graph_construction.py # Entities â†’ Graph database
â”‚   â”œâ”€â”€ 05_gnn_training.py       # Graph â†’ GNN training
â”‚   â”œâ”€â”€ 06_query_analysis.py     # Query â†’ Analysis
â”‚   â”œâ”€â”€ 07_unified_search.py     # Query â†’ Multi-modal search
â”‚   â”œâ”€â”€ 08_context_retrieval.py  # Results â†’ Context prep
â”‚   â”œâ”€â”€ 09_response_generation.py # Context â†’ Final response
â”‚   â”œâ”€â”€ 00_full_pipeline.py      # Complete processing orchestration
â”‚   â”œâ”€â”€ 10_query_pipeline.py     # Complete query orchestration
â”‚   â”œâ”€â”€ 11_streaming_monitor.py  # Real-time progress events
â”‚   â”œâ”€â”€ setup_azure_services.py  # Azure service initialization
â”‚   â”œâ”€â”€ demo_full_workflow.py    # End-to-end demonstration
â”‚   â””â”€â”€ README.md                # Data flow documentation
â”œâ”€â”€ legacy/                      # ğŸ“š REFERENCE - Function-based scripts  
â”‚   â”œâ”€â”€ rag_cli.py               # Unified CLI (reference)
â”‚   â”œâ”€â”€ data_pipeline.py         # Pipeline orchestrator (reference)
â”‚   â”œâ”€â”€ [other legacy scripts...]
â”‚   â””â”€â”€ README.md                # Legacy documentation
â””â”€â”€ README.md                    # This file
```

## ğŸ¯ **Philosophy: Scripts as Living Documentation**

**Previous Approach**: Scripts organized by function (CLI, demo, test)
- Obscured the actual data flow
- Difficult to understand README pipeline
- Generic processing without clear stages

**New Approach**: Scripts directly mirror README data flow
- âœ… Each script = one README pipeline stage
- âœ… Sequential numbering shows dependencies  
- âœ… Perfect demonstration of architecture
- âœ… Educational for new developers

## ğŸš€ **Quick Start**

### **Execute Complete Processing Pipeline**:
```bash
cd backend/scripts/dataflow
python 00_full_pipeline.py --source data/raw --domain general
```

### **Execute Individual Stages** (for testing/debugging):
```bash
python 01_data_ingestion.py --source data/raw
python 02_knowledge_extraction.py --domain general  
python 03_vector_indexing.py --index-name rag-index
# ... continue through stages
```

### **Execute Query Pipeline**:
```bash  
python 10_query_pipeline.py --query "How does maintenance work?"
```

### **Full Demonstration**:
```bash
python demo_full_workflow.py
```

## ğŸ“‹ **Makefile Integration**

The root Makefile routes to these data flow scripts:

```bash
make data-prep-full     # Executes dataflow/00_full_pipeline.py
make knowledge-extract  # Executes dataflow/02_knowledge_extraction.py  
make query-demo         # Executes dataflow/10_query_pipeline.py
```

## ğŸ“Š **Benefits**

1. **README Alignment**: Scripts directly demonstrate README architecture
2. **Educational Value**: Sequential execution teaches system flow
3. **Granular Control**: Test/debug individual pipeline stages
4. **Perfect Demos**: Show exact data flow to stakeholders
5. **Clear Dependencies**: Numbered stages show processing order
6. **Living Documentation**: Scripts serve as executable examples

**Result**: Backend scripts are now living, executable documentation of your Azure Universal RAG data flow! ğŸ‰