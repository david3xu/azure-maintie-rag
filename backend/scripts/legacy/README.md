# Legacy Scripts

**Purpose**: Historical scripts organized by function rather than data flow

**Status**: âš ï¸ **LEGACY** - Preserved for reference, replaced by data flow architecture

**Replaced By**: `../dataflow/` - Scripts that directly reflect README data flow stages

---

## ğŸ“ **Legacy Script Organization**

These scripts were organized by **function** (CLI, demo, test, etc.) rather than **data flow stages**:

```
legacy/
â”œâ”€â”€ rag_cli.py                    # Unified CLI entry point
â”œâ”€â”€ data_pipeline.py              # Data processing orchestrator
â”œâ”€â”€ demo_runner.py                # Demo execution
â”œâ”€â”€ azure_setup.py                # Azure configuration  
â”œâ”€â”€ gnn_trainer.py                # GNN training
â”œâ”€â”€ test_runner.py                # Test execution + validation
â”œâ”€â”€ workflow_runner.py            # Workflow execution + lifecycle
â”œâ”€â”€ azure_credentials_setup.sh    # Credential setup
â”œâ”€â”€ azure_ml_conda_env.yml        # ML environment
â””â”€â”€ SCRIPT_REFACTORING_PLAN.md    # Legacy architecture documentation
```

## ğŸ”„ **Why Replaced?**

**Issue**: Scripts didn't reflect the README data flow architecture
- Users couldn't see the processing pipeline stages
- No clear demonstration of: Raw Text â†’ Knowledge Extraction â†’ Vector/Graph â†’ GNN â†’ Query Processing
- Function-based organization obscured the actual data flow

**Solution**: New data flow-aligned scripts in `../dataflow/`

## ğŸš€ **Migration Path**

**Old Approach**:
```bash
python scripts/rag_cli.py data --mode full  # Generic data processing
python scripts/demo_runner.py               # Generic demo
```

**New Approach**:
```bash
python scripts/dataflow/01_data_ingestion.py     # Specific data flow stage
python scripts/dataflow/02_knowledge_extraction.py
python scripts/dataflow/03_vector_indexing.py
# ... sequential processing that matches README
```

## ğŸ“‹ **Preservation Reason**

These scripts contain working implementations that may be useful for reference:
- **Unified CLI pattern** from `rag_cli.py`
- **Service integration examples** from various runners
- **Azure setup patterns** from `azure_setup.py`
- **Working pipeline orchestration** from `data_pipeline.py`

**Recommendation**: Use `../dataflow/` scripts for development, reference these for implementation patterns.