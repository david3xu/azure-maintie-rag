#!/usr/bin/env python3
"""
Optimized Azure Cosmos DB Gremlin Bulk Loader
===========================================

Implements efficient bulk loading using Python asyncio and optimized batching
strategies to work within Azure Cosmos DB Gremlin API constraints.

Based on Azure Cosmos DB Gremlin Python limitations:
- No native bulk executor (unlike .NET/Java SDKs)
- No bytecode traversal support for bulk operations
- Individual API calls required for each entity/relationship

Optimizations implemented:
1. Asyncio concurrent processing
2. Intelligent batching with rate limiting
3. Connection pooling and reuse
4. Retry logic with exponential backoff
5. Progress tracking and monitoring
"""

import asyncio
import json
import time
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import logging

sys.path.append(str(Path(__file__).parent.parent))

from core.azure_cosmos.cosmos_gremlin_client import AzureCosmosGremlinClient

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class BulkLoadConfig:
    """Configuration for optimized bulk loading"""
    max_concurrent_requests: int = 10  # Concurrent async requests
    batch_size: int = 50  # Items per batch
    retry_attempts: int = 3
    base_delay: float = 1.0  # Base delay for exponential backoff
    rate_limit_delay: float = 0.1  # Delay between requests
    progress_interval: int = 100  # Progress reporting interval


class OptimizedCosmosBulkLoader:
    """Optimized bulk loader for Azure Cosmos DB Gremlin API"""
    
    def __init__(self, config: Optional[BulkLoadConfig] = None):
        self.config = config or BulkLoadConfig()
        self.client = AzureCosmosGremlinClient()
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def create_semaphore_pool(self) -> asyncio.Semaphore:
        """Create semaphore for controlling concurrent requests"""
        return asyncio.Semaphore(self.config.max_concurrent_requests)
    
    async def execute_with_retry(self, query: str, semaphore: asyncio.Semaphore) -> bool:
        """Execute Gremlin query with retry logic and rate limiting"""
        async with semaphore:
            for attempt in range(self.config.retry_attempts):
                try:
                    # Rate limiting
                    await asyncio.sleep(self.config.rate_limit_delay)
                    
                    # Execute query using the correct method
                    result = self.client._execute_gremlin_query_safe(query)
                    self.stats['successful'] += 1
                    return True
                    
                except Exception as e:
                    logger.warning(f"Attempt {attempt + 1} failed: {e}")
                    if attempt < self.config.retry_attempts - 1:
                        # Exponential backoff
                        delay = self.config.base_delay * (2 ** attempt)
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"Final attempt failed for query: {query[:100]}...")
                        self.stats['failed'] += 1
                        return False
    
    def create_entity_query(self, entity: Dict[str, Any]) -> str:
        """Create optimized Gremlin query for entity insertion"""
        entity_id = entity['entity_id']
        text = entity['text'].replace("'", "\\'")  # Escape quotes
        entity_type = entity['entity_type']
        
        # Optimized single-statement query with property setting
        query = f"""
        g.addV('{entity_type}')
         .property(id, '{entity_id}')
         .property('text', '{text}')
         .property('entity_type', '{entity_type}')
         .property('partition_key', '{entity_type}')
        """.strip().replace('\n', ' ')
        
        return query
    
    def create_relationship_query(self, relationship: Dict[str, Any]) -> str:
        """Create optimized Gremlin query for relationship insertion"""
        source_id = relationship['source_entity_id']
        target_id = relationship['target_entity_id']
        relation_type = relationship['relation_type']
        confidence = relationship.get('confidence', 1.0)
        
        # Optimized edge creation query
        query = f"""
        g.V('{source_id}').addE('{relation_type}').to(g.V('{target_id}'))
         .property('confidence', {confidence})
         .property('relation_type', '{relation_type}')
        """.strip().replace('\n', ' ')
        
        return query
    
    async def bulk_load_entities(self, entities: List[Dict[str, Any]]) -> Dict[str, int]:
        """Bulk load entities using async processing"""
        logger.info(f"Starting bulk entity loading: {len(entities)} entities")
        
        # Create semaphore for concurrency control
        semaphore = await self.create_semaphore_pool()
        
        # Create queries
        queries = [self.create_entity_query(entity) for entity in entities]
        
        # Execute queries concurrently
        tasks = [self.execute_with_retry(query, semaphore) for query in queries]
        
        # Process in batches to manage memory
        results = []
        for i in range(0, len(tasks), self.config.batch_size):
            batch = tasks[i:i + self.config.batch_size]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            results.extend(batch_results)
            
            # Progress reporting
            processed = min(i + self.config.batch_size, len(tasks))
            if processed % self.config.progress_interval == 0:
                logger.info(f"Entities processed: {processed}/{len(tasks)}")
        
        successful = sum(1 for r in results if r is True)
        failed = len(results) - successful
        
        logger.info(f"Entity loading completed: {successful} successful, {failed} failed")
        return {'successful': successful, 'failed': failed}
    
    async def bulk_load_relationships(self, relationships: List[Dict[str, Any]]) -> Dict[str, int]:
        """Bulk load relationships using async processing"""
        logger.info(f"Starting bulk relationship loading: {len(relationships)} relationships")
        
        # Create semaphore for concurrency control
        semaphore = await self.create_semaphore_pool()
        
        # Create queries
        queries = [self.create_relationship_query(rel) for rel in relationships]
        
        # Execute queries concurrently
        tasks = [self.execute_with_retry(query, semaphore) for query in queries]
        
        # Process in batches
        results = []
        for i in range(0, len(tasks), self.config.batch_size):
            batch = tasks[i:i + self.config.batch_size]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            results.extend(batch_results)
            
            # Progress reporting
            processed = min(i + self.config.batch_size, len(tasks))
            if processed % self.config.progress_interval == 0:
                logger.info(f"Relationships processed: {processed}/{len(tasks)}")
        
        successful = sum(1 for r in results if r is True)
        failed = len(results) - successful
        
        logger.info(f"Relationship loading completed: {successful} successful, {failed} failed")
        return {'successful': successful, 'failed': failed}
    
    async def bulk_load_complete_dataset(self, data_file: Path, entity_limit: Optional[int] = None) -> Dict[str, Any]:
        """Load complete dataset with entities and relationships"""
        logger.info(f"Loading complete dataset from: {data_file}")
        self.stats['start_time'] = time.time()
        
        # Load data
        with open(data_file, 'r') as f:
            data = json.load(f)
        
        entities = data['entities']
        relationships = data['relationships']
        
        # Apply entity limit if specified
        if entity_limit:
            entities = entities[:entity_limit]
            entity_ids = {entity['entity_id'] for entity in entities}
            relationships = [
                rel for rel in relationships 
                if rel['source_entity_id'] in entity_ids and rel['target_entity_id'] in entity_ids
            ]
            logger.info(f"Limited to {len(entities)} entities, {len(relationships)} relationships")
        
        # Load entities first (required for relationships)
        entity_results = await self.bulk_load_entities(entities)
        
        # Load relationships
        relationship_results = await self.bulk_load_relationships(relationships)
        
        self.stats['end_time'] = time.time()
        duration = self.stats['end_time'] - self.stats['start_time']
        
        # Compile final statistics
        total_successful = entity_results['successful'] + relationship_results['successful']
        total_failed = entity_results['failed'] + relationship_results['failed']
        total_processed = len(entities) + len(relationships)
        
        throughput = total_processed / duration if duration > 0 else 0
        
        results = {
            'entities': entity_results,
            'relationships': relationship_results,
            'totals': {
                'processed': total_processed,
                'successful': total_successful,
                'failed': total_failed,
                'success_rate': total_successful / total_processed if total_processed > 0 else 0
            },
            'performance': {
                'duration_seconds': duration,
                'throughput_items_per_second': throughput,
                'average_request_time': duration / total_processed if total_processed > 0 else 0
            },
            'configuration': {
                'max_concurrent_requests': self.config.max_concurrent_requests,
                'batch_size': self.config.batch_size,
                'rate_limit_delay': self.config.rate_limit_delay
            }
        }
        
        logger.info(f"Bulk loading completed:")
        logger.info(f"  Total processed: {total_processed}")
        logger.info(f"  Success rate: {results['totals']['success_rate']:.1%}")
        logger.info(f"  Duration: {duration:.1f}s")
        logger.info(f"  Throughput: {throughput:.2f} items/sec")
        
        return results


async def main():
    """Execute optimized bulk loading demonstration"""
    
    print("üöÄ OPTIMIZED AZURE COSMOS DB BULK LOADER")
    print("=" * 60)
    
    # Configuration for different scenarios
    configs = {
        'aggressive': BulkLoadConfig(
            max_concurrent_requests=20,
            batch_size=100,
            rate_limit_delay=0.05
        ),
        'balanced': BulkLoadConfig(
            max_concurrent_requests=10,
            batch_size=50,
            rate_limit_delay=0.1
        ),
        'conservative': BulkLoadConfig(
            max_concurrent_requests=5,
            batch_size=25,
            rate_limit_delay=0.2
        )
    }
    
    # Use balanced configuration
    config = configs['balanced']
    loader = OptimizedCosmosBulkLoader(config)
    
    # Data file
    data_file = Path(__file__).parent.parent / "data/extraction_outputs/full_dataset_extraction_9100_entities_5848_relationships.json"
    
    if not data_file.exists():
        print(f"‚ùå Data file not found: {data_file}")
        return 1
    
    try:
        # Test with subset first (1000 entities for performance validation)
        print(f"\nüîÑ Testing with subset (1000 entities)...")
        results = await loader.bulk_load_complete_dataset(data_file, entity_limit=1000)
        
        # Save results
        output_file = Path(__file__).parent.parent / "data/demo_outputs/optimized_bulk_loading_results.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\n‚úÖ Optimized bulk loading completed!")
        print(f"üìä Results saved to: {output_file}")
        print(f"üìà Performance: {results['performance']['throughput_items_per_second']:.2f} items/sec")
        print(f"‚úÖ Success rate: {results['totals']['success_rate']:.1%}")
        
        # Performance comparison
        original_throughput = 1.3  # items/sec from individual operations
        improvement = results['performance']['throughput_items_per_second'] / original_throughput
        print(f"üöÄ Performance improvement: {improvement:.1f}x faster than individual operations")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå Bulk loading failed: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))