Based on your actual codebase analysis, here's the comprehensive Azure Universal RAG data lifecycle workflow with implementation gap analysis.## **Azure Universal RAG: Complete Data Lifecycle Architecture**

Based on your actual codebase analysis, here's the comprehensive enterprise workflow with implementation gap assessment.

---

## **Complete Azure Data Lifecycle Workflow**

### **Phase A: GNN Model Training Pipeline** *(Periodic/Event-Driven)*

| **Step** | **Azure Service** | **Expected Action** | **Implementation Status** | **Gap Analysis** |
|----------|-------------------|-------------------|--------------------------|------------------|
| **A1** | **AzureServicesManager** | Raw data migration (`data/raw/*.md` â†’ Azure Services) | ğŸŸ¡ **PARTIAL** | `migrate_data_to_azure()` returns `{"success": False, "error": "Not implemented"}` |
| **A2** | **Azure Cosmos DB** | Graph export for training with quality validation | ğŸŸ¡ **PARTIAL** | `export_graph_for_training()` called but quality validation logic incomplete |
| **A3** | **Azure ML** | GNN training orchestration with incremental updates | ğŸŸ¡ **PARTIAL** | `orchestrate_incremental_training()` framework exists, monitoring incomplete |
| **A4** | **Azure ML** | Model quality assessment and deployment | ğŸŸ¡ **PARTIAL** | Quality assessor has placeholder methods, deployment partial |
| **A5** | **Azure Cosmos DB** | Update graph with pre-computed embeddings | ğŸ”´ **MISSING** | `_update_graph_embeddings()` method not implemented |

### **Phase B: Real-Time Query Processing Pipeline** *(Production Runtime)*

| **Step** | **Azure Service** | **Expected Action** | **Implementation Status** | **Gap Analysis** |
|----------|-------------------|-------------------|--------------------------|------------------|
| **B1** | **Text Processor** | Query text normalization and tokenization | âœ… **COMPLETE** | `{"clean_text": query, "tokens": query.split()}` |
| **B2** | **Azure OpenAI GPT-4** | Dynamic entity/relation extraction from query | âœ… **COMPLETE** | `extract_knowledge_from_texts()` implemented |
| **B3** | **Azure OpenAI Embeddings** | Vector embedding generation for documents | âœ… **COMPLETE** | `build_index_universal()` with FAISS integration |
| **B4** | **NetworkX + Azure ML** | Graph structure preparation for GNN processing | âœ… **COMPLETE** | `prepare_universal_gnn_data()` implemented |
| **B5** | **Query Analyzer** | Semantic query analysis and concept expansion | âœ… **COMPLETE** | `analyze_query_universal()` and `enhance_query_universal()` |
| **B6** | **Multi-Modal Search** | Vector + Graph + GNN hybrid search | ğŸŸ¡ **PARTIAL** | `enhance_search_results()` method missing implementation |
| **B7** | **Azure OpenAI GPT-4** | Context-aware response generation | âœ… **COMPLETE** | `generate_universal_response()` implemented |

---

## **Enterprise Architecture Data Flow**

### **Training Phase Data Flow** *(Environment-Driven)*
```
ğŸ“ Raw Data (data/raw/*.md)
    â†“ [AzureServicesManager.migrate_data_to_azure]
ğŸ”„ Azure Services Migration
    â”œâ”€â”€ Azure Blob Storage (documents)
    â”œâ”€â”€ Azure Cognitive Search (vector index)
    â””â”€â”€ Azure Cosmos DB (entities/relations)
    â†“ [cosmos_client.export_graph_for_training]
ğŸ“Š Graph Export & Quality Validation
    â†“ [AzureGNNTrainingOrchestrator.orchestrate_incremental_training]
ğŸ§  Azure ML GNN Training
    â†“ [Environment-specific thresholds: DEV=50, STAGING=100, PROD=200]
ğŸ¯ Model Deployment & Embedding Storage
    â†“ [Pre-computed embeddings â†’ Azure Cosmos DB]
âœ… Production-Ready GNN Model
```

### **Query Processing Data Flow** *(Real-Time)*
```
ğŸ” User Query ("How do I fix pump failure?")
    â†“ [7-Step Processing Pipeline]
ğŸ“‹ Query Analysis & Enhancement
    â†“ [Multi-modal search: Vector + Graph + GNN]
ğŸ” Search Results with GNN Enhancement
    â†“ [Azure OpenAI GPT-4 response generation]
ğŸ’¬ Final Response with Citations
```

---

## **Critical Implementation Gaps Analysis**

### **High-Priority Gaps** *(Blocking Production)*

#### **Gap 1: Azure Data Migration Pipeline**
**Location**: `backend/integrations/azure_services.py`
**Issue**:
```python
# Current implementation returns not implemented:
search_result = self._migrate_to_search(source_data_path, domain, migration_context) if hasattr(self, '_migrate_to_search') else {"success": False, "error": "Not implemented"}
```

**Required Fix**: Implement `_migrate_to_storage()`, `_migrate_to_search()`, `_migrate_to_cosmos()` methods

#### **Gap 2: GNN Search Enhancement**
**Location**: `backend/core/azure_ml/gnn_processor.py`
**Issue**: `enhance_search_results()` method called but not implemented
**Impact**: GNN capabilities not utilized in query processing

#### **Gap 3: Graph Change Metrics**
**Location**: `backend/core/azure_cosmos/enhanced_gremlin_client.py`
**Issue**: `get_graph_change_metrics()` referenced but not implemented
**Impact**: Incremental training triggers not functional

### **Medium-Priority Gaps** *(Performance/Quality)*

#### **Gap 4: Model Quality Assessment**
**Location**: `backend/core/azure_ml/gnn/model_quality_assessor.py`
**Issue**: Placeholder methods with hardcoded values
```python
def _assess_connectivity_understanding(self, model, data_loader) -> float:
    return 0.8  # Placeholder
```

#### **Gap 5: Training Progress Monitoring**
**Location**: `backend/core/azure_ml/gnn_orchestrator.py`
**Issue**: `_monitor_training_progress()` has incomplete error handling

### **Low-Priority Gaps** *(Enhancement)*

#### **Gap 6: Embedding Update Pipeline**
**Location**: `backend/scripts/orchestrate_gnn_pipeline.py`
**Issue**: `_update_entity_embeddings()` returns zero updates
**Impact**: Pre-computed embeddings not refreshed

---

## **Azure Services Integration Architecture**

### **Enterprise Service Dependencies**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AzureServicesManager                     â”‚
â”‚                  (Enterprise Orchestration)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
  â”‚Azure Blob â”‚ â”‚Azure Searchâ”‚ â”‚Azure Cosmosâ”‚
  â”‚Storage    â”‚ â”‚Cognitive   â”‚ â”‚DB Gremlin  â”‚
  â”‚Multi-Tier â”‚ â”‚Vector Indexâ”‚ â”‚Graph Store â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                              â”‚Azure ML   â”‚
                              â”‚GNN Trainingâ”‚
                              â”‚& Inference â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Environment-Specific Configuration** *(From your environment configs)*
```python
# Data-driven scaling by environment:
DEV:     GNN_TRAINING_TRIGGER_THRESHOLD=50,    AZURE_ML_COMPUTE_INSTANCES=1
STAGING: GNN_TRAINING_TRIGGER_THRESHOLD=100,   AZURE_ML_COMPUTE_INSTANCES=2
PROD:    GNN_TRAINING_TRIGGER_THRESHOLD=200,   AZURE_ML_COMPUTE_INSTANCES=4

# Quality gates by environment:
DEV:     GNN_QUALITY_THRESHOLD=0.6
STAGING: GNN_QUALITY_THRESHOLD=0.65
PROD:    GNN_QUALITY_THRESHOLD=0.7
```

---

## **Implementation Priority Recommendations**

### **Phase 1: Core Data Pipeline** *(Weeks 1-2)*
1. **Complete Azure migration methods** in `AzureServicesManager`
2. **Implement graph change metrics** in `EnterpriseGremlinGraphManager`
3. **Fix GNN search enhancement** in `AzureMLGNNProcessor`

### **Phase 2: Training Pipeline** *(Weeks 3-4)*
4. **Complete model quality assessment** with real metrics
5. **Implement embedding update pipeline**
6. **Add comprehensive training monitoring**

### **Phase 3: Performance Optimization** *(Weeks 5-6)*
7. **Add caching layers** for pre-computed embeddings
8. **Implement batch processing** for large datasets
9. **Add comprehensive telemetry** via Azure Application Insights

---

## **Enterprise Success Metrics**

### **Operational Excellence**
- **Training Pipeline SLA**: < 4 hours for incremental updates
- **Query Processing SLA**: < 2 seconds end-to-end
- **Cost Optimization**: Environment-specific resource allocation
- **Quality Gates**: Progressive quality thresholds by environment

### **Technical Performance**
- **GNN Model Quality**: > 0.7 in production environment
- **Search Enhancement**: > 20% improvement over vector-only search
- **Azure Integration**: 99.9% uptime across all services
- **Data Freshness**: Daily embedding updates with quality validation

This architecture positions your Universal RAG system as an enterprise-grade Azure-native solution with clear implementation priorities and measurable success criteria.


## **Phase 1: Azure Universal RAG Core Data Pipeline Architecture**

### **Enterprise Architecture Overview**

Phase 1 establishes the foundational Azure data orchestration layer, transforming your Universal RAG from local processing to cloud-native enterprise architecture. This implements the missing Azure services integration patterns identified in your codebase analysis.

---

## **Component 1: Azure Data Migration Orchestration Service**

### **Architectural Context**
**Location**: `backend/integrations/azure_services.py`
**Current Gap**: `{"success": False, "error": "Not implemented"}` across all migration methods
**Enterprise Pattern**: Multi-service data orchestration with transactional consistency

### **Service Architecture Design**

```python
class AzureDataMigrationOrchestrator:
    """
    Enterprise data migration orchestration across Azure services
    Implements transactional data consistency with rollback capabilities
    """
```

#### **Sub-Component Architecture**

**1. Storage Migration Service**
```python
async def _migrate_to_storage(self, source_path: str, domain: str, context: Dict) -> Dict[str, Any]:
    """
    Azure Blob Storage multi-container orchestration
    Pattern: Hierarchical namespace with domain-based partitioning
    """
```

**Design Considerations**:
- **Multi-Account Strategy**: Leverage your existing `storage_factory` from codebase
- **Container Naming**: Environment-driven (`universal-rag-data-{env}`)
- **Cost Optimization**: Tier-based storage (Hot/Cool/Archive by environment)
- **Security**: Azure Managed Identity integration

**2. Search Index Migration Service**
```python
async def _migrate_to_search(self, source_path: str, domain: str, context: Dict) -> Dict[str, Any]:
    """
    Azure Cognitive Search index orchestration with semantic capabilities
    Pattern: Domain-specific index creation with vector search optimization
    """
```

**Design Considerations**:
- **Index Strategy**: Dynamic index creation (`rag-index-{domain}`)
- **Schema Evolution**: Document schema management for universal types
- **Performance**: Environment-specific SKU allocation (basic/standard)
- **Monitoring**: Integration with Azure Application Insights

**3. Graph Migration Service**
```python
async def _migrate_to_cosmos(self, source_path: str, domain: str, context: Dict) -> Dict[str, Any]:
    """
    Azure Cosmos DB Gremlin graph population with partition optimization
    Pattern: Entity-relation graph construction with performance optimization
    """
```

**Design Considerations**:
- **Partition Strategy**: Domain-based partitioning for query optimization
- **Throughput Management**: Environment-driven RU allocation
- **Graph Modeling**: Universal entity/relation schema design
- **Consistency**: Strong consistency for training data integrity

### **Integration Architecture**

**Data Flow Orchestration**:
```
Raw Data â†’ Knowledge Extraction â†’ Parallel Azure Services Population
    â†“           â†“                    â†“
Blob Storage â† Search Index â† Cosmos Graph
    â†“           â†“                    â†“
Validation â†’ Consistency Check â†’ Transaction Commit/Rollback
```

**Monitoring & Observability**:
- Azure Application Insights telemetry integration
- Custom metrics for migration progress tracking
- Azure Service Health integration for dependency monitoring

---

## **Component 2: Graph Analytics & Change Detection Service**

### **Architectural Context**
**Location**: `backend/core/azure_cosmos/enhanced_gremlin_client.py`
**Current Gap**: `get_graph_change_metrics()` referenced but not implemented
**Enterprise Pattern**: Event-driven analytics with caching optimization

### **Service Architecture Design**

```python
class AzureGraphAnalyticsService:
    """
    Enterprise graph analytics with change detection capabilities
    Implements efficient Gremlin traversal patterns with Redis caching
    """
```

#### **Analytics Architecture Components**

**1. Change Detection Engine**
```python
async def get_graph_change_metrics(self, domain: str) -> Dict[str, Any]:
    """
    Graph change analytics with temporal tracking
    Pattern: Incremental change detection with threshold-based triggers
    """
```

**Design Considerations**:
- **Temporal Tracking**: Last processed timestamp management
- **Change Quantification**: Entity/relation delta calculations
- **Performance**: Gremlin query optimization for large graphs
- **Caching**: Azure Redis Cache integration for frequent queries

**2. Graph Quality Assessment Engine**
```python
async def assess_graph_training_quality(self, domain: str) -> Dict[str, Any]:
    """
    Graph structure quality validation for GNN training readiness
    Pattern: Multi-dimensional quality scoring with environment thresholds
    """
```

**Design Considerations**:
- **Quality Metrics**: Connectivity, diversity, completeness scoring
- **Environment Thresholds**: Data-driven quality gates per environment
- **Scalability**: Distributed graph analysis for large domains
- **Alerting**: Azure Monitor integration for quality degradation

#### **Gremlin Optimization Patterns**

**Query Optimization Strategy**:
- **Index Utilization**: Leverage Cosmos DB composite indexes
- **Traversal Efficiency**: Minimize graph traversal depth
- **Partitioning Awareness**: Domain-based query routing
- **Caching Strategy**: Multi-layer caching (application + Redis)

**Performance Monitoring**:
- Request Unit (RU) consumption tracking
- Query execution time monitoring
- Partition hot-spot detection
- Automatic query optimization recommendations

---

## **Component 3: GNN Search Enhancement Service**

### **Architectural Context**
**Location**: `backend/core/azure_ml/gnn_processor.py`
**Current Gap**: `enhance_search_results()` method called but not implemented
**Enterprise Pattern**: Hybrid search orchestration with ML integration

### **Service Architecture Design**

```python
class AzureGNNSearchEnhancementService:
    """
    Enterprise GNN-powered search enhancement
    Implements hybrid vector-graph search with pre-computed embeddings
    """
```

#### **Enhancement Architecture Components**

**1. Pre-Computed Embedding Retrieval Service**
```python
async def retrieve_precomputed_embeddings(self, entities: List[str], domain: str) -> Dict[str, np.ndarray]:
    """
    High-performance embedding retrieval from Cosmos DB
    Pattern: Batch retrieval with connection pooling optimization
    """
```

**Design Considerations**:
- **Connection Management**: Gremlin connection pooling for concurrency
- **Batch Optimization**: Vectorized embedding retrieval
- **Caching Strategy**: Multi-tier caching (memory + Redis + Cosmos)
- **Fallback Strategy**: Graceful degradation for missing embeddings

**2. Similarity Computation Engine**
```python
async def compute_gnn_similarity_scores(self, query_embeddings: np.ndarray, doc_embeddings: np.ndarray) -> List[float]:
    """
    Optimized similarity computation with Azure GPU acceleration
    Pattern: Vectorized computation with optional GPU acceleration
    """
```

**Design Considerations**:
- **Computation Optimization**: NumPy vectorization for CPU efficiency
- **GPU Acceleration**: Azure ML compute integration for large-scale similarity
- **Memory Management**: Streaming computation for large embedding sets
- **Cost Optimization**: Compute resource scaling based on query volume

**3. Search Result Orchestration Service**
```python
async def orchestrate_hybrid_search(self, vector_results: List[Dict], gnn_scores: List[float]) -> List[Dict]:
    """
    Hybrid search result ranking with configurable weighting
    Pattern: Multi-signal ranking with environment-specific tuning
    """
```

**Design Considerations**:
- **Ranking Strategy**: Configurable vector/GNN score weighting
- **Environment Tuning**: Performance-optimized weights per environment
- **Result Diversity**: Anti-clustering algorithms for result variety
- **Performance SLA**: Sub-second response time optimization

---

## **Azure Integration Architecture**

### **Service Mesh Design**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Azure API Management Gateway                 â”‚
â”‚           (Rate Limiting, Authentication, Monitoring)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚Data    â”‚    â”‚Graph        â”‚    â”‚GNN Search â”‚
â”‚Migrationâ”‚    â”‚Analytics    â”‚    â”‚Enhancementâ”‚
â”‚Service â”‚    â”‚Service      â”‚    â”‚Service    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚                 â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚Multi-   â”‚    â”‚Redis      â”‚     â”‚Azure ML   â”‚
â”‚Storage  â”‚    â”‚Cache      â”‚     â”‚Compute    â”‚
â”‚Factory  â”‚    â”‚Layer      â”‚     â”‚Clusters   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Security & Governance Architecture**

**Identity & Access Management**:
- Azure Managed Identity for service-to-service authentication
- Azure Key Vault integration for secrets management
- Role-Based Access Control (RBAC) for service boundaries
- Azure Policy enforcement for compliance requirements

**Monitoring & Observability**:
- Azure Application Insights distributed tracing
- Azure Monitor custom metrics and alerts
- Azure Service Health dependency monitoring
- Cost monitoring with Azure Cost Management integration

**Scalability & Performance**:
- Azure Autoscaling based on queue depth and CPU metrics
- Connection pooling optimization for Cosmos DB and Redis
- Circuit breaker patterns for downstream service resilience
- Performance testing with Azure Load Testing integration

---

## **Implementation Deployment Strategy**

### **Service Deployment Pattern**

**Phase 1A: Data Migration Orchestration**
- Deploy enhanced `AzureServicesManager` with migration capabilities
- Implement transactional consistency with rollback mechanisms
- Configure environment-specific service tiers and thresholds

**Phase 1B: Graph Analytics Service**
- Deploy `AzureGraphAnalyticsService` with change detection
- Implement Redis caching layer for performance optimization
- Configure Azure Monitor alerting for graph quality metrics

**Phase 1C: GNN Search Enhancement**
- Deploy `AzureGNNSearchEnhancementService` with hybrid ranking
- Implement pre-computed embedding retrieval optimization
- Configure environment-specific performance tuning parameters

### **Environment Progression Strategy**

**Development Environment**:
- Basic service tiers for cost optimization
- Enhanced logging for debugging and development
- Reduced caching TTL for rapid iteration

**Staging Environment**:
- Production-equivalent service tiers
- Full monitoring and alerting configuration
- Performance benchmarking and load testing

**Production Environment**:
- Premium service tiers for optimal performance
- Advanced security and compliance configurations
- Comprehensive disaster recovery and backup strategies

This enterprise architecture establishes your Universal RAG system as a scalable, secure, and cost-optimized Azure-native solution with clear separation of concerns and robust operational excellence patterns.

## **Phase 1: Azure Universal RAG Core Pipeline Implementation**

### **Implementation 1: Azure Data Migration Orchestration**

**File**: `backend/integrations/azure_services.py`
**Integration Point**: Existing `migrate_data_to_azure()` method framework

#### **Enterprise Migration Service Implementation**

```python
async def _migrate_to_storage(self, source_data_path: str, domain: str, migration_context: Dict[str, Any]) -> Dict[str, Any]:
    """Azure Blob Storage migration with multi-account orchestration"""
    try:
        from pathlib import Path
        import aiofiles

        # Use existing storage factory from codebase
        storage_client = self.get_rag_storage_client()
        if not storage_client:
            raise RuntimeError("RAG storage client not initialized")

        # Environment-driven container configuration
        container_name = f"{azure_settings.azure_blob_container}-{domain}"

        # Read raw data files with async pattern
        source_path = Path(source_data_path)
        if not source_path.exists():
            return {"success": False, "error": f"Source path not found: {source_data_path}"}

        uploaded_files = []
        failed_uploads = []

        for file_path in source_path.glob("*.md"):
            try:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()

                # Azure Blob naming with domain hierarchy
                blob_name = f"{domain}/{file_path.name}"

                # Upload with metadata tagging
                blob_metadata = {
                    "domain": domain,
                    "migration_id": migration_context["migration_id"],
                    "source_file": file_path.name,
                    "environment": azure_settings.azure_environment
                }

                upload_result = await storage_client.upload_blob_async(
                    container_name=container_name,
                    blob_name=blob_name,
                    data=content.encode('utf-8'),
                    metadata=blob_metadata,
                    overwrite=True
                )

                if upload_result.get("success", False):
                    uploaded_files.append(blob_name)
                else:
                    failed_uploads.append({"file": file_path.name, "error": upload_result.get("error")})

            except Exception as file_error:
                failed_uploads.append({"file": file_path.name, "error": str(file_error)})

        # Azure Application Insights telemetry
        if self.app_insights and self.app_insights.enabled:
            self.app_insights.track_event(
                name="azure_storage_migration",
                properties={
                    "domain": domain,
                    "migration_id": migration_context["migration_id"],
                    "container": container_name
                },
                measurements={
                    "files_uploaded": len(uploaded_files),
                    "files_failed": len(failed_uploads),
                    "duration_seconds": time.time() - migration_context["start_time"]
                }
            )

        return {
            "success": len(failed_uploads) == 0,
            "uploaded_files": uploaded_files,
            "failed_uploads": failed_uploads,
            "container_name": container_name,
            "total_files": len(uploaded_files) + len(failed_uploads)
        }

    except Exception as e:
        logger.error(f"Storage migration failed: {e}")
        return {"success": False, "error": str(e)}

async def _migrate_to_search(self, source_data_path: str, domain: str, migration_context: Dict[str, Any]) -> Dict[str, Any]:
    """Azure Cognitive Search index creation with vector capabilities"""
    try:
        # Use existing search service from codebase
        search_service = self.get_service('search')
        if not search_service:
            raise RuntimeError("Search service not initialized")

        # Environment-driven index configuration
        index_name = f"rag-index-{domain}"

        # Azure Cognitive Search schema with vector support
        index_schema = {
            "name": index_name,
            "fields": [
                {"name": "id", "type": "Edm.String", "key": True, "searchable": False},
                {"name": "content", "type": "Edm.String", "searchable": True, "analyzer": "standard.lucene"},
                {"name": "title", "type": "Edm.String", "searchable": True, "filterable": True},
                {"name": "domain", "type": "Edm.String", "filterable": True, "facetable": True},
                {"name": "source", "type": "Edm.String", "filterable": True},
                {"name": "contentVector", "type": "Collection(Edm.Single)", "searchable": True, "dimensions": 1536, "vectorSearchProfile": "vector-profile"}
            ],
            "vectorSearch": {
                "profiles": [
                    {
                        "name": "vector-profile",
                        "algorithm": "hnsw-config"
                    }
                ],
                "algorithms": [
                    {
                        "name": "hnsw-config",
                        "kind": "hnsw",
                        "hnswParameters": {
                            "metric": "cosine",
                            "m": 4,
                            "efConstruction": 400,
                            "efSearch": 500
                        }
                    }
                ]
            }
        }

        # Create index with environment-specific configuration
        index_result = await search_service.create_index_async(index_schema)

        if not index_result.get("success", False):
            return {"success": False, "error": f"Index creation failed: {index_result.get('error')}"}

        # Process documents for indexing
        from core.azure_openai.knowledge_extractor import AzureOpenAIKnowledgeExtractor

        # Read and extract knowledge from source files
        knowledge_extractor = AzureOpenAIKnowledgeExtractor(domain)

        source_path = Path(source_data_path)
        texts = []
        sources = []

        for file_path in source_path.glob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():
                        texts.append(content)
                        sources.append(str(file_path))
            except Exception as e:
                logger.warning(f"Failed to read {file_path}: {e}")

        if not texts:
            return {"success": False, "error": "No valid text content found for indexing"}

        # Extract knowledge using existing extractor
        extraction_results = await knowledge_extractor.extract_knowledge_from_texts(texts, sources)

        if not extraction_results.get("success", False):
            return {"success": False, "error": f"Knowledge extraction failed: {extraction_results.get('error')}"}

        # Prepare documents for Azure Cognitive Search
        knowledge_data = knowledge_extractor.get_extracted_knowledge()
        search_documents = []

        for doc_id, doc_data in knowledge_data["documents"].items():
            # Generate embeddings using existing Azure OpenAI integration
            content = doc_data["text"]

            # Use existing embedding service
            from core.azure_search.vector_service import AzureSearchVectorService
            vector_service = AzureSearchVectorService(domain)

            try:
                embedding = await vector_service._get_embedding(content)
                embedding_list = embedding.tolist() if hasattr(embedding, 'tolist') else list(embedding)
            except Exception as e:
                logger.warning(f"Embedding generation failed for {doc_id}: {e}")
                embedding_list = [0.0] * 1536  # Fallback zero vector

            search_doc = {
                "id": doc_id,
                "content": content,
                "title": doc_data.get("title", ""),
                "domain": domain,
                "source": doc_data.get("metadata", {}).get("source", "unknown"),
                "contentVector": embedding_list
            }
            search_documents.append(search_doc)

        # Upload documents to search index
        if search_documents:
            upload_result = await search_service.upload_documents_async(search_documents)
            indexed_count = upload_result.get("uploaded_count", 0)
        else:
            indexed_count = 0

        # Azure Application Insights telemetry
        if self.app_insights and self.app_insights.enabled:
            self.app_insights.track_event(
                name="azure_search_migration",
                properties={
                    "domain": domain,
                    "migration_id": migration_context["migration_id"],
                    "index_name": index_name
                },
                measurements={
                    "documents_indexed": indexed_count,
                    "extraction_entities": len(knowledge_data.get("entities", {})),
                    "extraction_relations": len(knowledge_data.get("relations", [])),
                    "duration_seconds": time.time() - migration_context["start_time"]
                }
            )

        return {
            "success": indexed_count > 0,
            "index_name": index_name,
            "documents_indexed": indexed_count,
            "entities_extracted": len(knowledge_data.get("entities", {})),
            "relations_extracted": len(knowledge_data.get("relations", []))
        }

    except Exception as e:
        logger.error(f"Search migration failed: {e}")
        return {"success": False, "error": str(e)}

async def _migrate_to_cosmos(self, source_data_path: str, domain: str, migration_context: Dict[str, Any]) -> Dict[str, Any]:
    """Azure Cosmos DB Gremlin graph population with entity/relation migration"""
    try:
        # Use existing Cosmos client from codebase
        cosmos_client = self.get_service('cosmos')
        if not cosmos_client:
            raise RuntimeError("Cosmos DB client not initialized")

        # Extract knowledge using existing pattern
        from core.azure_openai.knowledge_extractor import AzureOpenAIKnowledgeExtractor

        knowledge_extractor = AzureOpenAIKnowledgeExtractor(domain)

        # Read source files
        source_path = Path(source_data_path)
        texts = []
        sources = []

        for file_path in source_path.glob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if content.strip():
                        texts.append(content)
                        sources.append(str(file_path))
            except Exception as e:
                logger.warning(f"Failed to read {file_path}: {e}")

        if not texts:
            return {"success": False, "error": "No valid text content found for graph creation"}

        # Extract entities and relations
        extraction_results = await knowledge_extractor.extract_knowledge_from_texts(texts, sources)

        if not extraction_results.get("success", False):
            return {"success": False, "error": f"Knowledge extraction failed: {extraction_results.get('error')}"}

        knowledge_data = knowledge_extractor.get_extracted_knowledge()

        # Migrate entities to Cosmos DB
        entities_created = []
        entity_failures = []

        for entity_id, entity_data in knowledge_data["entities"].items():
            try:
                # Use existing entity addition pattern
                entity_creation_data = {
                    "id": entity_id,
                    "text": entity_data["text"],
                    "entity_type": entity_data["entity_type"],
                    "confidence": entity_data.get("confidence", 1.0),
                    "metadata": json.dumps({
                        "migration_id": migration_context["migration_id"],
                        "source": entity_data.get("source", "migration")
                    })
                }

                result = cosmos_client.add_entity(entity_creation_data, domain)

                if result.get("success", False):
                    entities_created.append(entity_id)
                else:
                    entity_failures.append({"entity_id": entity_id, "error": result.get("error")})

            except Exception as e:
                entity_failures.append({"entity_id": entity_id, "error": str(e)})

        # Migrate relations to Cosmos DB
        relations_created = []
        relation_failures = []

        for relation_data in knowledge_data["relations"]:
            try:
                # Use existing relation addition pattern
                relation_creation_data = {
                    "id": relation_data["relation_id"],
                    "head_entity": relation_data["head_entity"],
                    "tail_entity": relation_data["tail_entity"],
                    "relation_type": relation_data["relation_type"],
                    "confidence": relation_data.get("confidence", 1.0),
                    "created_at": datetime.now().isoformat()
                }

                result = cosmos_client.add_relationship(relation_creation_data, domain)

                if result.get("success", False):
                    relations_created.append(relation_data["relation_id"])
                else:
                    relation_failures.append({"relation_id": relation_data["relation_id"], "error": result.get("error")})

            except Exception as e:
                relation_failures.append({"relation_id": relation_data["relation_id"], "error": str(e)})

        # Validate graph statistics using existing method
        graph_stats = cosmos_client.get_graph_statistics(domain)
        validated_entities = graph_stats.get("vertex_count", 0)
        validated_relations = graph_stats.get("edge_count", 0)

        # Azure Application Insights telemetry
        if self.app_insights and self.app_insights.enabled:
            self.app_insights.track_event(
                name="azure_cosmos_migration",
                properties={
                    "domain": domain,
                    "migration_id": migration_context["migration_id"],
                    "database": azure_settings.azure_cosmos_database
                },
                measurements={
                    "entities_created": len(entities_created),
                    "relations_created": len(relations_created),
                    "validated_entities": validated_entities,
                    "validated_relations": validated_relations,
                    "duration_seconds": time.time() - migration_context["start_time"]
                }
            )

        return {
            "success": len(entity_failures) == 0 and len(relation_failures) == 0,
            "entities_created": entities_created,
            "relations_created": relations_created,
            "entity_failures": entity_failures,
            "relation_failures": relation_failures,
            "validated_entities": validated_entities,
            "validated_relations": validated_relations,
            "total_entities": len(entities_created) + len(entity_failures),
            "total_relations": len(relations_created) + len(relation_failures)
        }

    except Exception as e:
        logger.error(f"Cosmos migration failed: {e}")
        return {"success": False, "error": str(e)}
```

---

### **Implementation 2: Graph Analytics & Change Detection Service**

**File**: `backend/core/azure_cosmos/enhanced_gremlin_client.py`
**Integration Point**: Add missing `get_graph_change_metrics()` method

#### **Enterprise Graph Analytics Implementation**

```python
async def get_graph_change_metrics(self, domain: str, last_check_timestamp: Optional[str] = None) -> Dict[str, Any]:
    """Graph change detection with temporal analytics using Gremlin traversals"""
    try:
        if not self._client_initialized:
            self._initialize_client()

        # Environment-driven threshold configuration
        from config.settings import azure_settings
        trigger_threshold = getattr(azure_settings, 'gnn_training_trigger_threshold', 50)

        change_metrics = {
            "domain": domain,
            "analysis_timestamp": datetime.now().isoformat(),
            "trigger_threshold": trigger_threshold,
            "new_entities": 0,
            "new_relations": 0,
            "updated_entities": 0,
            "total_changes": 0,
            "requires_training": False
        }

        # Determine analysis timeframe
        if last_check_timestamp:
            # Incremental analysis since last check
            time_filter = f"and has('last_updated').where(gt('{last_check_timestamp}'))"
        else:
            # Full analysis - check entities created in last 24 hours as baseline
            from datetime import datetime, timedelta
            yesterday = (datetime.now() - timedelta(days=1)).isoformat()
            time_filter = f"and has('created_at').where(gt('{yesterday}'))"

        # Query new entities with temporal filtering
        new_entities_query = f"""
            g.V().has('domain', '{domain}')
                .has('label', 'Entity')
                {time_filter}
                .count()
        """

        try:
            new_entities_result = self._execute_gremlin_query_safe(new_entities_query, timeout_seconds=30)
            change_metrics["new_entities"] = new_entities_result[0] if new_entities_result else 0
        except Exception as e:
            logger.warning(f"New entities query failed: {e}")
            change_metrics["new_entities"] = 0

        # Query new relations with temporal filtering
        new_relations_query = f"""
            g.E().has('domain', '{domain}')
                {time_filter.replace('has(', 'has(') if time_filter else ''}
                .count()
        """

        try:
            new_relations_result = self._execute_gremlin_query_safe(new_relations_query, timeout_seconds=30)
            change_metrics["new_relations"] = new_relations_result[0] if new_relations_result else 0
        except Exception as e:
            logger.warning(f"New relations query failed: {e}")
            change_metrics["new_relations"] = 0

        # Query updated entities (entities with recent confidence score changes)
        updated_entities_query = f"""
            g.V().has('domain', '{domain}')
                .has('label', 'Entity')
                .has('confidence_updated_at')
                .where(has('confidence_updated_at', gt('{last_check_timestamp if last_check_timestamp else yesterday}')))
                .count()
        """

        try:
            updated_entities_result = self._execute_gremlin_query_safe(updated_entities_query, timeout_seconds=30)
            change_metrics["updated_entities"] = updated_entities_result[0] if updated_entities_result else 0
        except Exception as e:
            logger.warning(f"Updated entities query failed: {e}")
            change_metrics["updated_entities"] = 0

        # Calculate total changes and training requirement
        change_metrics["total_changes"] = (
            change_metrics["new_entities"] +
            change_metrics["new_relations"] +
            change_metrics["updated_entities"]
        )

        change_metrics["requires_training"] = change_metrics["total_changes"] >= trigger_threshold

        # Cache results using Redis if available
        try:
            cache_key = f"graph_change_metrics:{domain}:{change_metrics['analysis_timestamp'][:10]}"
            # Implement Redis caching if redis client is available
            if hasattr(self, '_redis_client') and self._redis_client:
                await self._redis_client.setex(
                    cache_key,
                    azure_settings.azure_data_state_cache_ttl,
                    json.dumps(change_metrics)
                )
        except Exception as e:
            logger.debug(f"Cache storage failed (non-critical): {e}")

        logger.info(f"Graph change analysis for {domain}: {change_metrics['total_changes']} total changes, training required: {change_metrics['requires_training']}")

        return change_metrics

    except Exception as e:
        logger.error(f"Graph change metrics analysis failed: {e}")
        return {
            "domain": domain,
            "analysis_timestamp": datetime.now().isoformat(),
            "error": str(e),
            "new_entities": 0,
            "new_relations": 0,
            "updated_entities": 0,
            "total_changes": 0,
            "requires_training": False
        }

async def assess_graph_training_quality(self, domain: str) -> Dict[str, Any]:
    """Comprehensive graph quality assessment for GNN training readiness"""
    try:
        if not self._client_initialized:
            self._initialize_client()

        # Environment-driven quality threshold
        from config.settings import azure_settings
        quality_threshold = getattr(azure_settings, 'gnn_quality_threshold', 0.6)

        quality_metrics = {
            "domain": domain,
            "assessment_timestamp": datetime.now().isoformat(),
            "quality_threshold": quality_threshold,
            "sufficient_for_training": False,
            "quality_score": 0.0,
            "recommendations": []
        }

        # 1. Entity count and diversity analysis
        entity_analysis_query = f"""
            g.V().has('domain', '{domain}')
                .has('label', 'Entity')
                .group()
                .by('entity_type')
                .by(count())
        """

        try:
            entity_types_result = self._execute_gremlin_query_safe(entity_analysis_query, timeout_seconds=30)
            entity_type_counts = entity_types_result[0] if entity_types_result else {}

            total_entities = sum(entity_type_counts.values()) if entity_type_counts else 0
            unique_entity_types = len(entity_type_counts) if entity_type_counts else 0

            quality_metrics["total_entities"] = total_entities
            quality_metrics["unique_entity_types"] = unique_entity_types
            quality_metrics["entity_type_distribution"] = entity_type_counts

        except Exception as e:
            logger.warning(f"Entity analysis failed: {e}")
            quality_metrics["total_entities"] = 0
            quality_metrics["unique_entity_types"] = 0

        # 2. Relation count and connectivity analysis
        relation_analysis_query = f"""
            g.E().has('domain', '{domain}')
                .group()
                .by('relation_type')
                .by(count())
        """

        try:
            relation_types_result = self._execute_gremlin_query_safe(relation_analysis_query, timeout_seconds=30)
            relation_type_counts = relation_types_result[0] if relation_types_result else {}

            total_relations = sum(relation_type_counts.values()) if relation_type_counts else 0
            unique_relation_types = len(relation_type_counts) if relation_type_counts else 0

            quality_metrics["total_relations"] = total_relations
            quality_metrics["unique_relation_types"] = unique_relation_types
            quality_metrics["relation_type_distribution"] = relation_type_counts

        except Exception as e:
            logger.warning(f"Relation analysis failed: {e}")
            quality_metrics["total_relations"] = 0
            quality_metrics["unique_relation_types"] = 0

        # 3. Graph connectivity analysis
        try:
            connectivity_ratio = (
                quality_metrics["total_relations"] / max(quality_metrics["total_entities"], 1)
            )
            quality_metrics["connectivity_ratio"] = connectivity_ratio
        except:
            quality_metrics["connectivity_ratio"] = 0.0

        # 4. Calculate composite quality score
        entity_score = min(1.0, quality_metrics["total_entities"] / 100.0)  # Normalize to 100 entities
        relation_score = min(1.0, quality_metrics["total_relations"] / 50.0)  # Normalize to 50 relations
        diversity_score = min(1.0, (quality_metrics["unique_entity_types"] + quality_metrics["unique_relation_types"]) / 20.0)
        connectivity_score = min(1.0, quality_metrics["connectivity_ratio"] / 2.0)  # Normalize to 2:1 ratio

        quality_metrics["quality_score"] = (
            entity_score * 0.3 +
            relation_score * 0.3 +
            diversity_score * 0.2 +
            connectivity_score * 0.2
        )

        # 5. Training readiness assessment
        min_entities = getattr(azure_settings, 'gnn_training_trigger_threshold', 50)
        min_relations = max(10, min_entities // 5)  # 20% of entity threshold

        quality_metrics["sufficient_for_training"] = (
            quality_metrics["total_entities"] >= min_entities and
            quality_metrics["total_relations"] >= min_relations and
            quality_metrics["quality_score"] >= quality_threshold
        )

        # 6. Generate recommendations
        recommendations = []

        if quality_metrics["total_entities"] < min_entities:
            recommendations.append(f"Insufficient entities: {quality_metrics['total_entities']} < {min_entities} required")

        if quality_metrics["total_relations"] < min_relations:
            recommendations.append(f"Insufficient relations: {quality_metrics['total_relations']} < {min_relations} required")

        if quality_metrics["quality_score"] < quality_threshold:
            recommendations.append(f"Quality score below threshold: {quality_metrics['quality_score']:.3f} < {quality_threshold}")

        if quality_metrics["connectivity_ratio"] < 0.5:
            recommendations.append("Low graph connectivity - consider adding more relationships")

        if quality_metrics["unique_entity_types"] < 3:
            recommendations.append("Low entity type diversity - consider broader content coverage")

        quality_metrics["recommendations"] = recommendations

        logger.info(f"Graph quality assessment for {domain}: score={quality_metrics['quality_score']:.3f}, sufficient={quality_metrics['sufficient_for_training']}")

        return quality_metrics

    except Exception as e:
        logger.error(f"Graph quality assessment failed: {e}")
        return {
            "domain": domain,
            "assessment_timestamp": datetime.now().isoformat(),
            "error": str(e),
            "sufficient_for_training": False,
            "quality_score": 0.0,
            "recommendations": [f"Quality assessment failed: {str(e)}"]
        }
```

---

### **Implementation 3: GNN Search Enhancement Service**

**File**: `backend/core/azure_ml/gnn_processor.py`
**Integration Point**: Implement missing `enhance_search_results()` method

#### **Enterprise GNN Enhancement Implementation**

```python
async def enhance_search_results(
    self,
    search_results: List[Dict[str, Any]],
    analysis_results: Dict[str, Any],
    knowledge_graph: Any = None
) -> List[Dict[str, Any]]:
    """
    Enterprise GNN-powered search enhancement using pre-computed embeddings
    Integrates with Azure Cosmos DB for embedding retrieval and similarity computation
    """

    if not search_results:
        logger.info("No search results to enhance")
        return search_results

    try:
        # Environment-driven enhancement configuration
        from config.settings import azure_settings
        enhancement_weight = 0.3  # GNN weight in hybrid scoring
        vector_weight = 0.7       # Vector search weight

        # Extract query entities from analysis results
        query_entities = analysis_results.get("entities_detected", [])
        query_concepts = analysis_results.get("concepts_detected", [])

        if not query_entities and not query_concepts:
            logger.info("No entities or concepts detected for GNN enhancement")
            return search_results

        logger.info(f"Enhancing {len(search_results)} search results using {len(query_entities)} entities and {len(query_concepts)} concepts")

        # Retrieve pre-computed GNN embeddings for query entities
        query_embeddings = await self._retrieve_query_embeddings(query_entities + query_concepts)

        if not query_embeddings:
            logger.info("No GNN embeddings found for query entities")
            return search_results

        # Enhance each search result
        enhanced_results = []

        for result in search_results:
            try:
                enhanced_result = result.copy()

                # Extract document entities (from search result metadata or content analysis)
                doc_entities = result.get("entities", [])
                doc_content = result.get("content", "")

                # If no entities in metadata, extract from content
                if not doc_entities and doc_content:
                    doc_entities = await self._extract_entities_from_content(doc_content)

                # Calculate GNN-based similarity
                gnn_similarity = await self._calculate_gnn_similarity(
                    query_embeddings,
                    doc_entities
                )

                # Compute hybrid score combining vector and GNN similarities
                original_score = result.get("score", 0.0)
                enhanced_score = (
                    original_score * vector_weight +
                    gnn_similarity * enhancement_weight
                )

                # Add enhancement metadata
                enhanced_result.update({
                    "gnn_similarity": gnn_similarity,
                    "enhanced_score": enhanced_score,
                    "original_score": original_score,
                    "enhancement_method": "gnn_pre_computed",
                    "doc_entities": doc_entities,
                    "enhancement_weight": enhancement_weight
                })

                enhanced_results.append(enhanced_result)

            except Exception as e:
                logger.warning(f"Failed to enhance result {result.get('doc_id', 'unknown')}: {e}")
                # Add original result without enhancement
                enhanced_results.append(result)

        # Re-rank results by enhanced scores
        enhanced_results.sort(key=lambda x: x.get("enhanced_score", x.get("score", 0.0)), reverse=True)

        # Log enhancement statistics
        original_scores = [r.get("score", 0.0) for r in search_results]
        enhanced_scores = [r.get("enhanced_score", r.get("score", 0.0)) for r in enhanced_results]

        avg_improvement = (
            (sum(enhanced_scores) / len(enhanced_scores)) -
            (sum(original_scores) / len(original_scores))
        ) if enhanced_scores and original_scores else 0.0

        logger.info(f"GNN enhancement completed: average score improvement = {avg_improvement:.4f}")

        return enhanced_results

    except Exception as e:
        logger.error(f"GNN search enhancement failed: {e}")
        # Return original results on enhancement failure
        return search_results

async def _retrieve_query_embeddings(self, entities: List[str]) -> Dict[str, np.ndarray]:
    """
    Retrieve pre-computed GNN embeddings from Azure Cosmos DB
    Uses batch querying for performance optimization
    """

    if not entities:
        return {}

    try:
        from core.azure_cosmos.cosmos_gremlin_client import AzureCosmosGremlinClient
        cosmos_client = AzureCosmosGremlinClient()

        embeddings = {}

        # Environment-driven embedding dimension
        from config.settings import azure_settings
        embedding_dim = getattr(azure_settings, 'graph_embedding_dimension', 128)

        # Batch query for embeddings (optimize for Cosmos DB RU consumption)
        batch_size = 10  # Process entities in batches
        entity_batches = [entities[i:i + batch_size] for i in range(0, len(entities), batch_size)]

        for batch in entity_batches:
            try:
                # Construct batch query for multiple entities
                entity_filters = " or ".join([f"has('text', '{entity}')" for entity in batch])

                batch_query = f"""
                    g.V().has('domain', '{self.domain}')
                        .where({entity_filters})
                        .project('text', 'embeddings')
                        .by('text')
                        .by('gnn_embeddings')
                """

                batch_results = cosmos_client._execute_gremlin_query_safe(batch_query, timeout_seconds=30)

                if batch_results:
                    for result in batch_results:
                        entity_text = result.get('text')
                        embedding_str = result.get('embeddings')

                        if entity_text and embedding_str:
                            try:
                                # Parse comma-separated embedding string
                                embedding_values = [float(x.strip()) for x in embedding_str.split(',')]

                                # Validate embedding dimension
                                if len(embedding_values) == embedding_dim:
                                    embeddings[entity_text] = np.array(embedding_values, dtype=np.float32)
                                else:
                                    logger.warning(f"Invalid embedding dimension for {entity_text}: {len(embedding_values)} != {embedding_dim}")

                            except (ValueError, AttributeError) as e:
                                logger.warning(f"Failed to parse embedding for {entity_text}: {e}")

            except Exception as e:
                logger.warning(f"Batch embedding retrieval failed: {e}")
                continue

        logger.info(f"Retrieved {len(embeddings)} GNN embeddings from {len(entities)} requested entities")
        return embeddings

    except Exception as e:
        logger.error(f"GNN embedding retrieval failed: {e}")
        return {}

async def _calculate_gnn_similarity(
    self,
    query_embeddings: Dict[str, np.ndarray],
    doc_entities: List[str]
) -> float:
    """
    Calculate similarity using pre-computed GNN embeddings
    Implements vectorized computation for performance
    """

    if not query_embeddings or not doc_entities:
        return 0.0

    try:
        # Retrieve embeddings for document entities
        doc_embeddings = await self._retrieve_query_embeddings(doc_entities)

        if not doc_embeddings:
            return 0.0

        # Convert to numpy arrays for vectorized computation
        query_vecs = list(query_embeddings.values())
        doc_vecs = list(doc_embeddings.values())

        if not query_vecs or not doc_vecs:
            return 0.0

        # Calculate average embeddings
        query_avg = np.mean(query_vecs, axis=0)
        doc_avg = np.mean(doc_vecs, axis=0)

        # Compute cosine similarity
        dot_product = np.dot(query_avg, doc_avg)
        query_norm = np.linalg.norm(query_avg)
        doc_norm = np.linalg.norm(doc_avg)

        if query_norm == 0 or doc_norm == 0:
            return 0.0

        similarity = dot_product / (query_norm * doc_norm)

        # Ensure similarity is in valid range [0, 1]
        similarity = max(0.0, min(1.0, (similarity + 1.0) / 2.0))

        return float(similarity)

    except Exception as e:
        logger.error(f"GNN similarity calculation failed: {e}")
        return 0.0

async def _extract_entities_from_content(self, content: str) -> List[str]:
    """
    Extract entities from document content using existing knowledge extractor
    Fallback method when entities are not available in search metadata
    """

    try:
        if not content or len(content.strip()) < 10:
            return []

        # Use existing Azure OpenAI knowledge extractor for entity extraction
        from core.azure_openai.knowledge_extractor import AzureOpenAIKnowledgeExtractor

        extractor = AzureOpenAIKnowledgeExtractor(self.domain)

        # Extract entities from content
        extraction_results = await extractor.extract_knowledge_from_texts([content], ["search_result"])

        if extraction_results.get("success", False):
            knowledge_data = extractor.get_extracted_knowledge()
            entities = list(knowledge_data.get("entities", {}).keys())

            # Extract entity texts
            entity_texts = []
            for entity_id, entity_data in knowledge_data.get("entities", {}).items():
                entity_text = entity_data.get("text", "")
                if entity_text:
                    entity_texts.append(entity_text)

            return entity_texts[:10]  # Limit to top 10 entities for performance
        else:
            return []

    except Exception as e:
        logger.warning(f"Entity extraction from content failed: {e}")
        return []
```

---

### **Azure Integration Architecture**

#### **Service Orchestration Pattern**

```python
# Azure Services Manager integration pattern
class AzureUniversalRAGOrchestrator:
    """Enterprise orchestration service combining all Phase 1 components"""

    def __init__(self):
        self.azure_services = AzureServicesManager()
        self.migration_service = AzureDataMigrationOrchestrator()
        self.graph_analytics = AzureGraphAnalyticsService()
        self.gnn_enhancement = AzureGNNSearchEnhancementService()

    async def orchestrate_complete_migration(self, domain: str) -> Dict[str, Any]:
        """Complete data migration orchestration across all Azure services"""

    async def orchestrate_gnn_training_pipeline(self, domain: str) -> Dict[str, Any]:
        """GNN training pipeline orchestration with quality gates"""

    async def orchestrate_enhanced_search(self, query: str, domain: str) -> Dict[str, Any]:
        """Enhanced search orchestration with GNN capabilities"""
```

#### **Monitoring & Telemetry Integration**

```python
# Azure Application Insights integration pattern
def track_enterprise_metrics(self, operation: str, domain: str, metrics: Dict[str, Any]):
    """Enterprise telemetry tracking across all Phase 1 services"""

    if self.app_insights and self.app_insights.enabled:
        self.app_insights.track_event(
            name=f"azure_universal_rag_{operation}",
            properties={
                "domain": domain,
                "environment": azure_settings.azure_environment,
                "service_tier": azure_settings.azure_search_sku
            },
            measurements=metrics
        )
```

This implementation provides enterprise-grade Azure integration with comprehensive error handling, performance optimization, and monitoring capabilities across all Phase 1 components.


