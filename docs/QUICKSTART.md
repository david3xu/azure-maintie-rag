# Azure Universal RAG - Quick Start Guide

**Universal RAG Setup - Based on Actual Implementation**

Get the Azure Universal RAG system with **zero hardcoded domain bias** running based on the actual codebase structure and Universal RAG philosophy.

## üîç Prerequisites

Based on the Universal RAG implementation:

### **Required Tools**
```bash
# Python 3.11+ (required for PydanticAI)
python --version  # Should be Python 3.11+

# Azure CLI for authentication  
az --version      # Verify Azure CLI installed
az login          # Authenticate with Azure

# Azure Developer CLI for infrastructure
curl -fsSL https://aka.ms/install-azd.sh | bash
azd --version     # Verify azd installed

# Node.js for React frontend
node --version    # Should be Node.js 18+ for React 19.1.0
```

### **Azure Services Required**
Universal RAG implementation uses real Azure services (no mocks):
- **Azure OpenAI** (AsyncAzureOpenAI with GPT-4 deployment)
- **Azure Cognitive Search** (vector search with 1536D embeddings)
- **Azure Cosmos DB** (Gremlin API for knowledge graphs)
- **Azure ML** (GNN training/inference with PyTorch)
- **Azure Blob Storage** (document management)
- **Azure Key Vault** (secrets management with DefaultAzureCredential)

## ‚ö° Universal RAG Implementation Setup

### 1. Environment Configuration (2 minutes)

```bash
# Clone and navigate
git clone <repository-url>
cd azure-maintie-rag

# Install Python dependencies (includes PydanticAI)
pip install -r requirements.txt

# Environment synchronization (critical for multi-environment)
./scripts/deployment/sync-env.sh prod           # Switch to production environment (default)
make sync-env                                   # Sync backend configuration

# Set up Azure environment variables (DefaultAzureCredential)
export AZURE_OPENAI_ENDPOINT="your-endpoint"
export AZURE_OPENAI_API_VERSION="2024-08-01-preview"  
export OPENAI_MODEL_DEPLOYMENT="your-deployment-name"
export USE_MANAGED_IDENTITY="false"  # Development mode
```

### 2. Deploy Azure Infrastructure (5 minutes)

```bash
# One-command Azure infrastructure deployment
azd up  # Deploys all 9 Azure services automatically

# Expected results:
# ‚úÖ Azure OpenAI (GPT-4 deployment)
# ‚úÖ Azure Cognitive Search (vector search)
# ‚úÖ Azure Cosmos DB (Gremlin API)
# ‚úÖ Azure ML (GNN training)  
# ‚úÖ Azure Blob Storage (documents)
# ‚úÖ Azure Key Vault (secrets)
# ‚úÖ Azure Application Insights (monitoring)
# ‚úÖ Azure Log Analytics (logging)
# ‚úÖ Azure Container Apps (hosting)
```

### 3. Verify Universal RAG Implementation (1 minute)

Test the actual Universal RAG components:

```bash
# Verify universal models (domain-agnostic)
python -c "
from agents.core.universal_models import UniversalDomainAnalysis
print('‚úÖ Universal models loaded (work for ANY domain)')

# Test Domain Intelligence Agent (content discovery, not classification)
python -c "
from agents.domain_intelligence.agent import get_domain_intelligence_agent
agent = get_domain_intelligence_agent()
print('‚úÖ Domain Intelligence Agent created')
# Expected: Discovers content characteristics without domain assumptions"
from agents.core.data_models import ExtractionQualityOutput, ValidatedEntity
print('‚úÖ Centralized data models loaded (80+ Pydantic models)')

# Check PydanticAI agents
from agents.domain_intelligence.agent import get_domain_intelligence_agent
from agents.knowledge_extraction.agent import get_knowledge_extraction_agent
from agents.universal_search.agent import get_universal_search_agent
print('‚úÖ Three PydanticAI agents available')
"
```

### 3. Test Real Azure Integration (2 minutes)

```bash
# Test actual Azure service initialization
python -c "
import asyncio
from agents.core.azure_service_container import ConsolidatedAzureServices

async def test_services():
    container = ConsolidatedAzureServices()
    print('üîß Testing real Azure service initialization...')
    
    try:
        status = await container.initialize_all_services()
        print('Azure Service Status:')
        for service, success in status.items():
            icon = '‚úÖ' if success else '‚ùå'
            print(f'{icon} {service}: {\"CONNECTED\" if success else \"FAILED\"}')
        
        health = container.get_service_status()
        print(f'Overall Health: {health[\"overall_health\"]}')
        
    except Exception as e:
        print(f'‚ö†Ô∏è Service initialization: {e}')
        print('Note: Requires real Azure service endpoints configured')

asyncio.run(test_services())
"
```

## üöÄ Running the System

### Start the FastAPI Backend

Based on `api/main.py` (42 lines):

```bash
# Start the FastAPI application
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Or use the make command if available
make dev
```

**What you'll get:**
- FastAPI app titled "Azure Universal RAG API"
- CORS middleware with wildcard origins
- Root endpoint at `/` showing version and available endpoints
- Health check at `/health`
- Search endpoints at `/api/v1/search`

### Test Real Endpoints

```bash
# Test root endpoint
curl http://localhost:8000/

# Test health check  
curl http://localhost:8000/health

# Test search endpoint (if implemented)
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{"query": "test query"}'
```

## üîß Real Agent Testing

### Domain Intelligence Agent (122 lines)

```bash
python -c "
from agents.domain_intelligence.agent import create_domain_intelligence_agent

try:
    agent = create_domain_intelligence_agent()
    print('‚úÖ Domain Intelligence Agent created successfully')
    print(f'   - Model: Uses get_azure_openai_model() from environment')
    print(f'   - Deps: DomainDeps with lazy initialization')
    print(f'   - Toolsets: domain_intelligence_toolset (FunctionToolset pattern)')
except Exception as e:
    print(f'‚ùå Domain Intelligence Agent: {e}')
"
```

### Knowledge Extraction Agent (368 lines)

```bash
python -c "
from agents.knowledge_extraction.agent import get_knowledge_extraction_agent

try:
    agent = get_knowledge_extraction_agent()
    print('‚úÖ Knowledge Extraction Agent created successfully')
    print(f'   - Implementation: _create_agent_with_toolset()')
    print(f'   - Model: OpenAIModel with AzureProvider')
    print(f'   - Features: Multi-strategy extraction with unified processor')
except Exception as e:
    print(f'‚ùå Knowledge Extraction Agent: {e}')
"
```

### Universal Search Agent (271 lines)

```bash
python -c "
from agents.universal_search.agent import get_universal_search_agent

try:
    agent = get_universal_search_agent()
    print('‚úÖ Universal Search Agent created successfully')
    print(f'   - Implementation: _create_agent_with_consolidated_orchestrator()')
    print(f'   - Features: Tri-modal search with consolidated orchestrator')
    print(f'   - Dependencies: UniversalSearchDeps')
except Exception as e:
    print(f'‚ùå Universal Search Agent: {e}')
"
```

## üìä Real Data Verification

Check the actual test corpus:

```bash
# Check for real data files
ls -la data/raw/azure-ai-services-language-service_output/ | head -10

# Count actual data files (should be 179 files)
find data/raw/azure-ai-services-language-service_output/ -name "*.md" | wc -l

# Sample content from actual files
head -5 data/raw/azure-ai-services-language-service_output/*.md | head -20
```

## üß™ Shared Infrastructure Testing  

Test the actual shared utilities:

```bash
python -c "
# Test text statistics utility (PydanticAI-enhanced)
from agents.shared.text_statistics import TextStatistics, calculate_text_statistics

text = 'This is a sample text for statistical analysis testing.'
stats = calculate_text_statistics(text)
print('‚úÖ Text Statistics utility working')
print(f'   Words: {stats.total_words}, Readability: {stats.readability_score:.1f}')

# Test content preprocessing
from agents.shared.content_preprocessing import clean_text_content, TextCleaningOptions

options = TextCleaningOptions(remove_html=True, normalize_whitespace=True)  
result = clean_text_content('<p>Sample HTML content</p>', options)
print('‚úÖ Content preprocessing working')
print(f'   Quality score: {result.cleaning_quality_score:.2f}')

print('‚úÖ Shared infrastructure operational')
"
```

## üîç Configuration Verification

Test the real configuration system:

```bash
python -c "
# Test centralized configuration functions
from config.universal_config import get_universal_config
from agents.core.simple_config_manager import SimpleConfigManager

try:
    system_config = get_system_config()
    print('‚úÖ System configuration loaded')
    
    model_config = get_model_config_bootstrap()
    print('‚úÖ Model configuration (bootstrap) loaded')
    print(f'   API Version: {model_config.api_version}')
    
except Exception as e:
    print(f'‚ö†Ô∏è Configuration: {e}')
    
# Test Azure settings
from config.settings import azure_settings
print('‚úÖ Azure settings loaded')
print(f'   OpenAI Endpoint: {azure_settings.azure_openai_endpoint or \"Not configured\"}')
"
```

## ‚ùå Troubleshooting

### Common Issues

**1. Missing Azure Credentials:**
```bash
az login
# Set environment variables for your Azure services
```

**2. Missing Dependencies (PydanticAI Import Errors):**
```bash
# Clean all local data and caches first
make clean-all

# Reinstall all dependencies 
pip install -r requirements.txt

# Verify PydanticAI installation
pip show pydantic-ai
python -c "import pydantic_ai; print('‚úÖ PydanticAI working')"
```

**3. Configuration Errors:**
```bash
# Check that azure_settings can load your endpoints
python -c "from config.settings import azure_settings; print(azure_settings.azure_openai_endpoint)"
```

**4. System Cleanup Commands:**
```bash
# Clean current session and logs only
make clean

# Clean ALL local data and caches (preserves data/raw/ and Azure services)
make clean-all

# Clean ALL Azure services data (Cosmos DB, Storage, Search indexes)
make dataflow-cleanup

# Complete fresh start (clean everything + reinstall)
make clean-all && pip install -r requirements.txt
```

## ‚úÖ Success Indicators

When everything is working correctly:

- **Azure Service Container**: ConsolidatedAzureServices initializes without errors  
- **PydanticAI Agents**: All three agents create successfully with proper dependencies
- **FastAPI Server**: Starts on port 8000 with health check responding
- **Configuration**: Centralized config functions load without circular dependency issues
- **Shared Utilities**: Text statistics and content preprocessing work correctly

## üåä 6-Phase Dataflow Pipeline

The Azure Universal RAG system includes a comprehensive 6-phase dataflow pipeline that processes real data through real Azure services:

### **Quick Dataflow Commands**

```bash
# Execute individual phases (optimal order)
make dataflow-cleanup     # Phase 0: Clean all Azure services (fresh start)
make dataflow-validate    # Phase 1: Validate all 3 PydanticAI agents
make dataflow-ingest      # Phase 2: Upload real data to Azure Storage
make dataflow-extract     # Phase 3: Extract knowledge and build graphs
make dataflow-integrate   # Phase 5: Full pipeline integration testing
make dataflow-query       # Phase 4: Query analysis and universal search
make dataflow-advanced    # Phase 6: GNN training and monitoring

# Execute complete pipeline (auto-runs cleanup first)
make dataflow-full        # Run all phases: 0‚Üí1‚Üí2‚Üí3‚Üí5‚Üí4‚Üí6

# Manual cleanup if needed
make dataflow-cleanup     # Phase 0: Clean all Azure services only
```

### **Phase 0 - Azure Services Cleanup (1 minute)**

Ensures clean start by removing all previous Azure service data:

```bash
make dataflow-cleanup

# What it does:
# üßπ Cleans Azure Storage blob containers (documents, processed data)
# üîç Cleans Azure Cognitive Search indexes (removes all documents)
# üï∏Ô∏è Cleans Azure Cosmos DB knowledge graphs (removes vertices & edges)
# üìä Preserves Azure service infrastructure (services stay operational)
# ‚úÖ Provides fresh environment for reliable pipeline execution
```

### **Phase 1 - Agent Validation (2 minutes)**

Validates all 3 PydanticAI agents with real Azure services:

```bash
make dataflow-validate

# What it does:
# ‚úÖ Tests Domain Intelligence Agent with real Azure AI documentation
# ‚úÖ Tests Knowledge Extraction Agent with entity/relationship extraction
# ‚úÖ Tests Universal Search Agent with tri-modal search capabilities
# ‚úÖ Uses real data from data/raw/azure-ai-services-language-service_output/
# ‚úÖ Reports actual performance metrics and success rates
```

### **Phase 2 - Data Ingestion (3 minutes)**

Uploads real Azure AI documentation to Azure services:

```bash
make dataflow-ingest

# What it does:
# üì§ Uploads 5 Azure AI Language Service documents to Azure Blob Storage
# üî¢ Creates 1536-dimensional vector embeddings using Azure OpenAI
# üîç Indexes documents in Azure Cognitive Search with vector support
# üìä Reports actual upload sizes and indexing statistics
```

### **Phase 3 - Knowledge Extraction (5 minutes)**

Extracts entities and relationships, builds knowledge graphs:

```bash
make dataflow-extract

# What it does:
# üß† Runs Knowledge Extraction Agent on real Azure AI documentation
# üï∏Ô∏è Builds knowledge graphs in Azure Cosmos DB (Gremlin API)
# üìà Creates entity and relationship networks
# üíæ Stores structured knowledge for graph neural network training
```

### **Phase 4 - Query Pipeline (3 minutes)**

Tests universal search with real queries:

```bash
make dataflow-query

# What it does:
# üîç Runs query analysis on real search terms
# üéØ Demonstrates tri-modal search (Vector + Graph + GNN)
# üìä Shows actual search results and confidence scores
# ‚ö° Reports real query response times and accuracy
```

### **Phase 5 - Integration Testing (5 minutes)**

Validates end-to-end pipeline integration:

```bash
make dataflow-integrate

# What it does:
# üîÑ Executes complete document-to-query pipeline
# üìã Demonstrates query generation showcase
# üß™ Tests inter-agent communication and data flow
# üìà Validates production readiness with real metrics
```

### **Phase 6 - Advanced Features (10 minutes)**

Demonstrates advanced capabilities:

```bash
make dataflow-advanced

# What it does:
# ü§ñ Trains Graph Neural Networks on real knowledge graphs
# üìä Sets up real-time monitoring and streaming
# ‚öôÔ∏è Demonstrates configuration system adaptability
# üöÄ Shows production-scale feature capabilities
```

### **Complete Pipeline Execution**

Run the entire 6-phase pipeline in optimal logical order:

```bash
# Complete execution (25-30 minutes)
make dataflow-full

# Execution order optimized for clean start and data dependencies:
# Phase 0 ‚Üí Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí Phase 5 ‚Üí Phase 4 ‚Üí Phase 6
# (Cleanup first, then Integration before Query to ensure knowledge graphs are built)

# Expected results (corrected execution order):
# ‚úÖ Phase 0: Azure services cleaned for fresh start
# ‚úÖ Phase 1: All 3 PydanticAI agents validated with real Azure services
# ‚úÖ Phase 2: Real Azure AI documentation processed (5 files, ~52KB)  
# ‚úÖ Phase 3: Knowledge graphs built in Azure Cosmos DB
# ‚úÖ Phase 5: End-to-end integration validated with data pipeline
# ‚úÖ Phase 4: Query pipeline tested with populated knowledge graphs
# ‚úÖ Phase 6: GNN models trained on real relationship data
```

### **Session Management & Cleanup**

All dataflow commands create detailed session reports:

```bash
# View current session report
make session-report

# Check session logs
ls -la logs/

# Clean previous sessions (logs only)
make clean
```

### **System Cleanup Options**

When encountering issues or starting fresh:

```bash
# 1. Clean current session and logs only
make clean

# 2. Clean ALL local data and caches (keeps data/raw/ and Azure services)
make clean-all

# 3. Clean ALL Azure services data (Cosmos DB, Storage, Search indexes)
make dataflow-cleanup

# 4. Complete fresh start (everything + reinstall dependencies)
make clean-all && pip install -r requirements.txt

# 5. Reset Azure services and start pipeline fresh
make dataflow-cleanup && make dataflow-full
```

**What each cleanup removes:**
- `make clean`: Session logs, Python cache files only
- `make clean-all`: All local processing results, preserves original data and Azure
- `make dataflow-cleanup`: All Azure service data (Storage blobs, Search indexes, Cosmos DB graphs)

**Always preserved:**
- Original data in `data/raw/azure-ai-services-language-service_output/`
- Azure service infrastructure (services stay operational)
- Core codebase and configuration files

## üß™ Real Data Validation

The dataflow pipeline processes actual Azure AI Language Service documentation:

```bash
# Check real data corpus
ls -la data/raw/azure-ai-services-language-service_output/
# Shows: azure-ai-services-language-service_part_*.md files

# Sample real content
head -10 data/raw/azure-ai-services-language-service_output/azure-ai-services-language-service_part_81.md
# Real Azure AI training documentation

# Count actual files being processed
find data/raw/ -name "*.md" | wc -l
# Output: 5 real Azure AI documentation files
```

## üéØ Next Steps

1. **Start with Fresh Environment**: `make dataflow-cleanup` - Clean Azure services for fresh start
2. **Validate All Agents**: `make dataflow-validate` - Test all 3 PydanticAI agents with real Azure
3. **Run Complete Pipeline**: `make dataflow-full` - Full system validation (auto-runs cleanup first)
4. **Explore Architecture**: Review `docs/ARCHITECTURE.md` for implementation details
5. **Development Workflow**: See `docs/DEVELOPMENT_GUIDE.md` for development patterns  
6. **Troubleshooting**: Check `docs/TROUBLESHOOTING.md` for Azure service issues

This system represents a **real, production-ready implementation** with genuine Azure service integration, PydanticAI agents, and comprehensive data models processing actual Azure AI documentation - no mock components or sample data.