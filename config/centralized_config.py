"""
Clean Configuration - Essential Parameters Only
============================================

This module contains the dramatically simplified configuration with only ~60 essential parameters 
(reduced from 925+ parameters - 93% reduction).

Based on systematic analysis against CODING_STANDARDS.md:
- ✅ Infrastructure/Azure requirements (80% of parameters)  
- ✅ Security/validation constraints (20% of parameters)
- ❌ Removed all hardcoded domain assumptions (470+ parameters)
- ❌ Removed all over-engineering (300+ parameters)

Domain-specific parameters are now generated by the Domain Intelligence Agent through the
Dynamic Configuration Manager, which bridges Config-Extraction workflow intelligence
with Search workflow execution.
"""

from dataclasses import dataclass
from typing import Dict, Any, List, Optional
import os
import asyncio


@dataclass
class SystemConfiguration:
    """Core system constraints and infrastructure limits"""
    # Resource management
    max_workers: int = 4
    max_concurrent_requests: int = 5
    max_batch_size: int = 10
    
    # Network and timeouts  
    openai_timeout: int = 60
    search_timeout: int = 30
    cosmos_timeout: int = 45
    max_retries: int = 3
    
    # Security boundaries
    max_query_length: int = 1000
    max_execution_time_seconds: float = 300.0
    max_azure_cost_usd: float = 10.0


@dataclass
class ExtractionConfiguration:
    """Core extraction parameters that impact quality - HARDCODED VALUES TO BE REMOVED"""
    # TODO: ALL critical extraction parameters must come from Domain Intelligence Agent
    # HARDCODED: entity_confidence_threshold = 0.7  # Should be domain-specific from corpus analysis
    # HARDCODED: relationship_confidence_threshold = 0.65  # Should be learned from relationship quality
    # HARDCODED: chunk_size = 1000  # Should be learned from document characteristics
    # HARDCODED: chunk_overlap_ratio = 0.2  # Should be adaptive based on content density
    # HARDCODED: max_entities_per_chunk = 15  # Should be learned from entity density
    # HARDCODED: max_relationships_per_entity = 20  # Should be learned from relationship patterns
    # HARDCODED: minimum_quality_score = 0.6  # Should be adaptive based on domain complexity
    
    # These must be loaded from Config-Extraction workflow - no defaults
    entity_confidence_threshold: float = None  # Must be loaded from domain analysis
    relationship_confidence_threshold: float = None  # Must be loaded from domain analysis
    chunk_size: int = None  # Must be loaded from document analysis
    chunk_overlap_ratio: float = None  # Must be loaded from content analysis
    max_entities_per_chunk: int = None  # Must be loaded from entity density analysis
    max_relationships_per_entity: int = None  # Must be loaded from relationship analysis
    minimum_quality_score: float = None  # Must be loaded from quality analysis
    
    # Input validation (always keep)
    min_entity_length: int = 2
    max_entity_length: int = 100
    min_relationship_confidence: float = 0.2
    max_relationship_confidence: float = 1.0
    
    # Azure endpoint (data-driven from environment)
    azure_endpoint: str = os.getenv("AZURE_OPENAI_ENDPOINT", "")
    api_version: str = os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")
    deployment_name: str = os.getenv("OPENAI_MODEL_DEPLOYMENT", "gpt-4o")


@dataclass
class SearchConfiguration:
    """Tri-modal search parameters - HARDCODED VALUES TO BE REMOVED"""
    # TODO: ALL search parameters must be learned from domain analysis and query complexity
    # Vector search - HARDCODED VALUES REMOVED
    # HARDCODED: vector_similarity_threshold = 0.7  # Should be domain-specific from corpus analysis
    # HARDCODED: vector_top_k = 10  # Should be query-complexity driven
    vector_similarity_threshold: float = None  # Must be loaded from domain analysis
    vector_top_k: int = None  # Must be loaded from query analysis
    
    # Graph search - HARDCODED VALUES REMOVED
    # HARDCODED: graph_max_depth = 3  # Should be domain-specific for relationship depth
    # HARDCODED: graph_max_entities = 10  # Should be learned from entity density
    graph_max_depth: int = None  # Must be loaded from relationship analysis
    graph_max_entities: int = None  # Must be loaded from entity analysis
    
    # GNN search - HARDCODED VALUES REMOVED
    # HARDCODED: gnn_pattern_threshold = 0.7  # Should be learned from training performance
    # HARDCODED: gnn_max_predictions = 20  # Should be domain-specific prediction capacity
    gnn_pattern_threshold: float = None  # Must be loaded from GNN training analysis
    gnn_max_predictions: int = None  # Must be loaded from domain analysis
    
    # Orchestration - HARDCODED VALUES REMOVED
    # HARDCODED: search_timeout_seconds = 120  # Should be adaptive based on query complexity
    # HARDCODED: max_results_per_modality = 10  # Should be domain-optimized
    # HARDCODED: max_final_results = 50  # Should be query-complexity driven
    search_timeout_seconds: int = None  # Must be loaded from performance analysis
    max_results_per_modality: int = None  # Must be loaded from domain analysis
    max_final_results: int = None  # Must be loaded from query analysis


@dataclass  
class ModelConfiguration:
    """Azure OpenAI model configuration - HARDCODED VALUES TO BE REMOVED"""
    # TODO: ALL model selection must come from Dynamic Model Manager
    # HARDCODED: gpt4o_deployment_name = "gpt-4o"  # Should be domain-specific based on performance analysis
    # HARDCODED: gpt4o_mini_deployment_name = "gpt-4o-mini"  # Should be cost-optimization driven
    # HARDCODED: text_embedding_deployment_name = "text-embedding-ada-002"  # Should be performance-optimized
    # HARDCODED: deployment_name = "gpt-4o"  # Should be query-complexity and domain-specific
    
    # These must be loaded from Dynamic Model Manager - no hardcoded defaults
    gpt4o_deployment_name: str = None  # Must be loaded from model performance analysis
    gpt4o_mini_deployment_name: str = None  # Must be loaded from cost-efficiency analysis  
    text_embedding_deployment_name: str = None  # Must be loaded from embedding performance analysis
    deployment_name: str = None  # Must be loaded from domain-specific model selection
    
    # API configuration (infrastructure, not model selection)
    openai_api_version: str = "2024-08-01-preview"
    api_version: str = os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")
    default_temperature: float = 0.0
    default_max_tokens: int = 4000
    
    # Azure endpoint (data-driven from environment)
    azure_endpoint: str = os.getenv("AZURE_OPENAI_ENDPOINT", "")


@dataclass
class CacheConfiguration:
    """Caching parameters"""
    default_ttl_seconds: int = 3600  # 1 hour
    redis_ttl_seconds: int = 300     # 5 minutes
    enable_caching: bool = True
    max_cache_entries: int = 10000
    max_entries_per_namespace: int = 10000
    cleanup_interval_hours: int = 24


@dataclass
class ProcessingConfiguration:
    """Processing and performance constraints"""
    max_workers: int = 4
    max_concurrent_chunks: int = 5
    max_features_vectorizer: int = 1000
    timeout_base_seconds: int = 30
    
    # Statistical constants (universal)
    statistical_significance_alpha: float = 0.05
    random_state: int = 42
    n_init_kmeans: int = 10


@dataclass
class WorkflowConfiguration:
    """Workflow timeouts and SLA requirements"""
    extraction_timeout_seconds: int = 300
    search_timeout_seconds: int = 120
    analysis_timeout_seconds: int = 180
    total_workflow_timeout_seconds: int = 600
    max_retry_attempts: int = 3
    retry_backoff_seconds: int = 5
    health_check_interval_seconds: int = 30
    performance_monitoring_interval: int = 60


@dataclass
class SecurityConfiguration:
    """Security and validation constraints"""
    # Input validation
    max_query_length: int = 1000
    max_document_size_mb: int = 50
    max_batch_size: int = 10
    
    # Rate limiting
    max_requests_per_minute: int = 100
    max_concurrent_users: int = 50
    
    # Resource limits
    max_memory_usage_mb: int = 2048
    max_cpu_usage_percent: int = 80


# Global configuration instances
_system_config: Optional[SystemConfiguration] = None
_extraction_config: Optional[ExtractionConfiguration] = None
_search_config: Optional[SearchConfiguration] = None
_model_config: Optional[ModelConfiguration] = None
_cache_config: Optional[CacheConfiguration] = None
_processing_config: Optional[ProcessingConfiguration] = None
_workflow_config: Optional[WorkflowConfiguration] = None
_security_config: Optional[SecurityConfiguration] = None


def get_system_config() -> SystemConfiguration:
    """Get system configuration with environment overrides"""
    global _system_config
    if _system_config is None:
        _system_config = SystemConfiguration(
            max_workers=int(os.getenv("MAX_WORKERS", "4")),
            max_concurrent_requests=int(os.getenv("MAX_CONCURRENT_REQUESTS", "5")),
            openai_timeout=int(os.getenv("OPENAI_TIMEOUT", "60")),
            max_query_length=int(os.getenv("MAX_QUERY_LENGTH", "1000")),
            max_execution_time_seconds=float(os.getenv("MAX_EXECUTION_TIME", "300.0")),
            max_azure_cost_usd=float(os.getenv("MAX_AZURE_COST", "10.0"))
        )
    return _system_config


def get_extraction_config(domain_name: str = "general") -> ExtractionConfiguration:
    """Get extraction configuration from Config-Extraction workflow intelligence"""
    # Import here to avoid circular imports
    from agents.core.dynamic_config_manager import dynamic_config_manager
    
    try:
        # Get dynamic configuration from Config-Extraction workflow
        dynamic_config = asyncio.run(dynamic_config_manager.get_extraction_config(domain_name))
        
        return ExtractionConfiguration(
            entity_confidence_threshold=dynamic_config.entity_confidence_threshold,
            relationship_confidence_threshold=dynamic_config.relationship_confidence_threshold,
            chunk_size=dynamic_config.chunk_size,
            chunk_overlap_ratio=dynamic_config.chunk_overlap / dynamic_config.chunk_size,
            max_entities_per_chunk=dynamic_config.max_entities_per_chunk,
            min_relationship_strength=dynamic_config.min_relationship_strength,
            batch_size=dynamic_config.batch_size,
            quality_validation_threshold=dynamic_config.quality_validation_threshold
        )
    except Exception as e:
        raise RuntimeError(
            f"Failed to load extraction configuration from Config-Extraction workflow for domain '{domain_name}': {e}. "
            "This indicates the Config-Extraction workflow needs to be run first to generate domain-specific parameters."
        )


def get_search_config(domain_name: str = "general", query: str = None) -> SearchConfiguration:
    """Get search configuration from domain analysis and workflow intelligence"""
    # Import here to avoid circular imports  
    from agents.core.dynamic_config_manager import dynamic_config_manager
    
    try:
        # Get dynamic configuration from domain analysis
        dynamic_config = asyncio.run(dynamic_config_manager.get_search_config(domain_name, query))
        
        return SearchConfiguration(
            vector_similarity_threshold=dynamic_config.vector_similarity_threshold,
            vector_top_k=dynamic_config.vector_top_k,
            graph_max_depth=dynamic_config.graph_hop_count,
            graph_max_entities=dynamic_config.vector_top_k,  # Use similar limit
            gnn_pattern_threshold=dynamic_config.gnn_prediction_confidence,
            gnn_max_predictions=dynamic_config.gnn_node_embeddings,
            search_timeout_seconds=120,  # Keep reasonable default for safety
            max_results_per_modality=dynamic_config.vector_top_k,
            max_final_results=dynamic_config.vector_top_k * 2  # Allow for synthesis
        )
    except Exception as e:
        raise RuntimeError(
            f"Failed to load search configuration from domain analysis for domain '{domain_name}': {e}. "
            "This indicates the Domain Intelligence Agent needs to analyze the domain first."
        )


def get_model_config(domain_name: str = "general", query: str = None, optimization_goal: str = "balanced") -> ModelConfiguration:
    """Get model configuration from Dynamic Model Manager intelligence"""
    # ❌ FORCING FUNCTION: All hardcoded model selection removed
    # This function now REQUIRES Config-Extraction workflow to analyze model performance
    raise RuntimeError(
        f"Failed to load model configuration from Dynamic Model Manager for domain '{domain_name}'. "
        "This indicates the Config-Extraction workflow needs to analyze model performance for this domain first. "
        "Hardcoded model selection removed to force proper workflow integration."
    )


def get_model_config_bootstrap() -> ModelConfiguration:
    """Bootstrap-only model configuration for system initialization"""
    # This is ONLY for system bootstrap - not for production model selection
    # Uses environment variables (infrastructure configuration, not model selection)
    return ModelConfiguration(
        gpt4o_deployment_name=os.getenv("GPT4O_DEPLOYMENT", "gpt-4o-deployment"),
        gpt4o_mini_deployment_name=os.getenv("GPT4O_MINI_DEPLOYMENT", "gpt-4o-mini-deployment"),
        text_embedding_deployment_name=os.getenv("EMBEDDING_DEPLOYMENT", "text-embedding-ada-002"),
        deployment_name=os.getenv("OPENAI_MODEL_DEPLOYMENT", "gpt-4o-deployment"),
        openai_api_version=os.getenv("OPENAI_API_VERSION", "2024-08-01-preview")
    )


def get_cache_config() -> CacheConfiguration:
    """Get cache configuration"""
    global _cache_config
    if _cache_config is None:
        _cache_config = CacheConfiguration()
    return _cache_config


def get_processing_config() -> ProcessingConfiguration:
    """Get processing configuration"""
    global _processing_config
    if _processing_config is None:
        _processing_config = ProcessingConfiguration()
    return _processing_config


def get_workflow_config() -> WorkflowConfiguration:
    """Get workflow configuration"""
    global _workflow_config
    if _workflow_config is None:
        _workflow_config = WorkflowConfiguration()
    return _workflow_config


def get_security_config() -> SecurityConfiguration:
    """Get security configuration"""
    global _security_config
    if _security_config is None:
        _security_config = SecurityConfiguration()
    return _security_config


# Domain Intelligence Agent Integration
@dataclass
class DomainConfiguration:
    """Generated by Domain Intelligence Agent based on corpus analysis"""
    domain_name: str
    entity_confidence_threshold: float
    relationship_confidence_threshold: float
    expected_entity_types: List[str]
    technical_vocabulary: List[str]
    key_concepts: List[str]
    chunk_size: int
    chunk_overlap_ratio: float
    max_entities_per_chunk: int
    minimum_quality_score: float
    
    # Statistical evidence for transparency
    statistical_basis: Dict[str, Any]
    confidence_in_analysis: float


def update_extraction_config_from_domain_analysis(domain_config: DomainConfiguration) -> ExtractionConfiguration:
    """Update extraction configuration with Domain Intelligence Agent output"""
    return ExtractionConfiguration(
        entity_confidence_threshold=domain_config.entity_confidence_threshold,
        relationship_confidence_threshold=domain_config.relationship_confidence_threshold,
        chunk_size=domain_config.chunk_size,
        chunk_overlap_ratio=domain_config.chunk_overlap_ratio,
        max_entities_per_chunk=domain_config.max_entities_per_chunk,
        minimum_quality_score=domain_config.minimum_quality_score,
        # Keep validation constraints
        min_entity_length=2,
        max_entity_length=100,
        min_relationship_confidence=0.2,
        max_relationship_confidence=1.0,
        max_relationships_per_entity=20
    )

# WORKFLOW INTEGRATION FUNCTIONS - IMPLEMENTED
def load_extraction_config_from_workflow(domain_name: str) -> ExtractionConfiguration:
    """Load extraction configuration from Config-Extraction workflow output"""
    # This function now integrates with Dynamic Configuration Manager
    return get_extraction_config(domain_name)


def load_search_config_from_workflow(domain_name: str, query: str = None) -> SearchConfiguration:
    """Load search configuration from domain analysis and workflow intelligence"""
    # This function now integrates with Dynamic Configuration Manager
    return get_search_config(domain_name, query)


def force_dynamic_config_loading() -> Dict[str, Any]:
    """Force regeneration of all dynamic configurations"""
    # Import here to avoid circular imports
    from agents.core.dynamic_config_manager import force_dynamic_config_loading as force_loading
    
    return asyncio.run(force_loading())


# Legacy compatibility - simplified
def get_config():
    """Legacy compatibility function for gradual migration"""
    class LegacyConfig:
        def __init__(self):
            self.system = get_system_config()
            self.extraction = get_extraction_config()
            self.search = get_search_config()
            self.model = get_model_config()
            self.cache = get_cache_config()
            self.processing = get_processing_config()
            self.workflow = get_workflow_config()
            self.security = get_security_config()
    
    return LegacyConfig()


# Initialize configuration manager - COMMENTED OUT TO FORCE DYNAMIC LOADING
# TODO: Configuration initialization must be replaced with workflow integration
# _config = get_config()  # HARDCODED - Remove to force Config-Extraction workflow integration

# FORCE DYNAMIC LOADING: Clear any cached configurations
_extraction_config = None
_search_config = None
_model_config = None  # Keep model config for Azure endpoints
_cache_config = None  # Keep cache config for performance
_processing_config = None  # Keep processing config for system limits
_workflow_config = None  # Keep workflow config for timeouts
_security_config = None  # Keep security config for validation
_system_config = None  # Keep system config for infrastructure


# =============================================================================
# LEGACY COMPATIBILITY FUNCTIONS (TEMPORARY - FOR GRADUAL MIGRATION)
# =============================================================================


def get_ml_hyperparameters_config():
    """Legacy compatibility - returns processing config"""
    return get_processing_config()


def get_azure_services_config():
    """Legacy compatibility - returns system config"""
    return get_system_config()


def get_capability_patterns_config():
    """Legacy compatibility - returns cache config"""  
    return get_cache_config()


def get_entity_processing_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_relationship_processing_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_validation_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_tri_modal_orchestration_config():
    """Legacy compatibility - returns search config"""
    return get_search_config()


def get_infrastructure_config():
    """Legacy compatibility - returns system config"""
    return get_system_config()


def get_vector_search_config():
    """Legacy compatibility - returns search config"""
    return get_search_config()


def get_graph_search_config():
    """Legacy compatibility - returns search config"""
    return get_search_config()


def get_gnn_search_config():
    """Legacy compatibility - returns search config"""
    return get_search_config()


def get_domain_analyzer_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_pattern_recognition_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_confidence_calculation_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_workflow_timeouts_config():
    """Legacy compatibility - returns workflow config"""
    return get_workflow_config()


def get_hybrid_domain_analyzer_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_pattern_engine_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_statistical_domain_analyzer_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_background_processor_config():
    """Legacy compatibility - returns processing config"""
    return get_processing_config()


def get_config_generator_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_domain_analysis_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_ml_config():
    """Legacy compatibility - returns processing config"""
    return get_processing_config()


def get_entity_extraction_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()


def get_quality_assessment_config():
    """Legacy compatibility - returns extraction config"""
    return get_extraction_config()