# Azure Developer CLI (azd) configuration for Universal RAG System
name: azure-maintie-rag
metadata:
  template: azure-search-openai-demo@main
  description: "Azure Universal RAG system with knowledge graphs, vector search, and GNN training"

# Services configuration for azd deployment
services:
  backend:
    project: .
    language: python
    host: containerapp
  frontend:
    project: ./frontend
    language: js
    host: containerapp

# Infrastructure configuration
infra:
  provider: bicep
  path: ./infra

# Environment variables that will be set by azd
env:
  # Azure Resource Configuration
  AZURE_LOCATION: westus2
  AZURE_RESOURCE_GROUP_PREFIX: rg-maintie-rag
  
  # Application Configuration
  OPENAI_MODEL_DEPLOYMENT: gpt-4o
  EMBEDDING_MODEL_DEPLOYMENT: text-embedding-ada-002
  SEARCH_INDEX_NAME: maintie-index
  COSMOS_DATABASE_NAME: maintie-rag-db
  COSMOS_GRAPH_NAME: knowledge-graph
  
  # Container Configuration
  BACKEND_PORT: 8000
  
  # Data Pipeline Configuration
  AUTO_POPULATE_DATA: true  # Set to false to skip automated data pipeline

# Deployment hooks
hooks:
  preprovision:
    shell: sh
    run: |
      echo "ğŸ—ï¸ Preparing Azure Universal RAG deployment..."
      echo "Environment: ${AZURE_ENV_NAME}"
      echo "Location: ${AZURE_LOCATION}"
      
  postprovision:
    shell: sh
    run: |
      echo "âœ… Infrastructure provisioned successfully!"
      echo ""
      echo "ğŸ” Checking deployed resources..."
      
      # Get resource group info
      RESOURCE_GROUP="rg-maintie-rag-${AZURE_ENV_NAME}"
      echo "Resource Group: $RESOURCE_GROUP"
      
      # List key resources
      echo ""
      echo "ğŸ“¦ Key Resources:"
      az resource list --resource-group "$RESOURCE_GROUP" --query "[].{Name:name, Type:type}" -o table 2>/dev/null || echo "Unable to list resources"
      
      echo ""
      
      # Check if automated data pipeline is enabled
      if [ "${AUTO_POPULATE_DATA:-true}" != "true" ]; then
        echo "â­ï¸  Automated data pipeline disabled (AUTO_POPULATE_DATA=false)"
        echo "ğŸ’¡ Run 'make dataflow-full' manually to populate data"
        echo ""
        echo "âœ… Infrastructure ready for manual data population"
        exit 0
      fi
      
      echo "ğŸ§¹ Starting automated data pipeline preparation..."
      
      # Set environment variables for Python scripts
      export PYTHONPATH=$PWD
      export USE_MANAGED_IDENTITY=false
      export OPENBLAS_NUM_THREADS=1
      
      # Phase 0: Cleanup existing data in Azure services
      echo "ğŸ“‹ Phase 0: Cleaning up existing Azure data..."
      if python scripts/dataflow/phase0_cleanup/00_01_cleanup_all_services.py; then
        echo "âœ… Phase 0: Data cleanup completed"
      else
        echo "âš ï¸  Phase 0: Cleanup failed, continuing anyway"
      fi
      
      # Phase 1: Validate all agents are working
      echo "ğŸ“‹ Phase 1: Validating PydanticAI agents..."
      if python scripts/dataflow/phase1_validation/01_01_validate_domain_intelligence.py && \
         python scripts/dataflow/phase1_validation/01_02_validate_knowledge_extraction.py && \
         python scripts/dataflow/phase1_validation/01_03_validate_universal_search.py; then
        echo "âœ… Phase 1: All 3 agents validated successfully"
      else
        echo "âŒ Phase 1: Agent validation failed - manual intervention required"
        exit 1
      fi
      
      # Phase 2: Upload documents and create embeddings
      echo "ğŸ“‹ Phase 2: Uploading Azure AI documents and creating embeddings..."
      if python scripts/dataflow/phase2_ingestion/02_02_storage_upload_primary.py && \
         python scripts/dataflow/phase2_ingestion/02_03_vector_embeddings.py && \
         python scripts/dataflow/phase2_ingestion/02_04_search_indexing.py; then
        echo "âœ… Phase 2: Document ingestion completed"
      else
        echo "âŒ Phase 2: Document ingestion failed - manual intervention required"
        exit 1
      fi
      
      # Phase 3: Run Agent 1 (Domain Intelligence) to prepare for tri-modal search
      echo "ğŸ“‹ Phase 3: Running Agent 1 (Domain Intelligence) to build database..."
      if python scripts/dataflow/phase3_knowledge/03_01_basic_entity_extraction.py && \
         python scripts/dataflow/phase3_knowledge/03_02_graph_storage.py; then
        echo "âœ… Phase 3: Knowledge extraction and graph storage completed"
        echo "ğŸ¯ Agent 1 has prepared the database - tri-modal search is ready!"
      else
        echo "âŒ Phase 3: Knowledge extraction failed - manual intervention required"
        exit 1
      fi
      
      # Phase 6: GNN training and model endpoint configuration
      echo "ğŸ“‹ Phase 6: Training GNN models and configuring endpoints..."
      if python scripts/dataflow/phase6_advanced/06_01_gnn_training.py && \
         python scripts/dataflow/phase6_advanced/06_10_gnn_deployment_pipeline.py; then
        echo "âœ… Phase 6: GNN training and deployment completed"
        echo "ğŸ§  GNN models trained and endpoints configured for tri-modal search!"
        
        # Configure GNN environment variables for the deployed system
        echo "ğŸ“‹ Configuring GNN endpoint environment variables..."
        
        # Extract GNN endpoint details from deployment pipeline output
        if [ -f "gnn_deployment_result.json" ]; then
          GNN_ENDPOINT=$(cat gnn_deployment_result.json | python -c "import sys, json; print(json.load(sys.stdin).get('endpoint_name', 'gnn-endpoint'))")
          GNN_SCORING_URI=$(cat gnn_deployment_result.json | python -c "import sys, json; print(json.load(sys.stdin).get('scoring_uri', ''))")
          
          # Set in azd environment for container apps
          azd env set GNN_ENDPOINT_NAME "$GNN_ENDPOINT"
          azd env set GNN_SCORING_URI "$GNN_SCORING_URI" 
          azd env set GNN_MODEL_VERSION "latest"
          
          echo "âœ… GNN environment variables configured:"
          echo "   GNN_ENDPOINT_NAME=$GNN_ENDPOINT"
          echo "   GNN_SCORING_URI=$GNN_SCORING_URI"
        else
          echo "âŒ GNN deployment result not found - pipeline failed"
          exit 1
        fi
      else
        echo "âŒ Phase 6: GNN training failed - manual intervention required"
        exit 1
      fi
      
      echo ""
      echo "ğŸ‰ AUTOMATED PIPELINE COMPLETED SUCCESSFULLY!"
      echo "âœ… Infrastructure deployed and configured"
      echo "âœ… All Azure services cleaned and populated with fresh data"  
      echo "âœ… Agent 1 has analyzed all documents and built knowledge database"
      echo "âœ… Knowledge graph stored in Cosmos DB with entity relationships"
      echo "âœ… GNN models trained and deployed to Azure ML endpoints"
      echo "âœ… Complete tri-modal search (Vector + Graph + GNN) ready for user queries"
      echo ""
      echo "ğŸ’¡ The system is now fully ready for production use!"
      echo "ğŸ’¡ User queries will skip Agent 1 and use pre-analyzed data for fast responses"
      echo "ğŸ’¡ All 3 search modalities (Vector, Graph, GNN) are operational"
      
  postdeploy:
    shell: sh
    run: |
      echo "âœ… Deployment complete!"
      echo ""
      echo "ğŸ”— Getting deployment URLs..."
      
      # Run our working URL script
      if [ -f "./scripts/show-deployment-urls.sh" ]; then
        ./scripts/show-deployment-urls.sh
      else
        echo "ğŸ“ Use this command to get URLs: ./scripts/show-deployment-urls.sh"
      fi
      
      echo ""
      echo "ğŸ“Š Environment: ${AZURE_ENV_NAME}"
      echo "ğŸ’¡ Chat interface is ready for testing!"